{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d413ac43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import amrlib\n",
    "stog = amrlib.load_stog_model()\n",
    "\n",
    "# print(\"Student Responses:\")\n",
    "# graphs = stog.parse_sents([\n",
    "# \"To show that a certain part of the program works as it is supposed to.\"\n",
    "# ])\n",
    "# for graph in graphs:\n",
    "#     print(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8c863c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/varishgrover/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63577214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "def generate_adjlist_text(sub, graphs1):\n",
    "    a = [i.replace(')',\"\") for i in graphs1[0].split(\"\\n\")]\n",
    "    b = [str((len(i) - len(i.lstrip()))//6)+i.strip() for i in a][1:]\n",
    "    c = [re.split(\"  | \", j) for j in [i.replace(\"/\",\"\") for i in [i.replace(\"(\",\"\") for i in b]]]\n",
    "    c[0] = [c[0][0][0], c[0][0][1:]]\n",
    "    d = {}\n",
    "    for i in c:\n",
    "        if len(i) == 3:\n",
    "            d[i[1]] = i[2]\n",
    "\n",
    "    c = [k for k in c]\n",
    "#     print(d, end = '\\n\\n\\n')\n",
    "    f = dict([[i, []] for i in set([w[1] for w in c])])\n",
    "    for i in range(len(c)):\n",
    "        if sub in c[i][0]:\n",
    "            j = i - 1\n",
    "            while j >=0 :\n",
    "                if c[j][0][0] < c[i][0][0]:\n",
    "                    break\n",
    "                j-=1\n",
    "            f[c[j][1]].append(c[i][1])\n",
    "            f[c[i][1]].append(c[j][1])\n",
    "    s = set()\n",
    "    for i in f.keys():\n",
    "        if len(f[i])!=0:\n",
    "            s.add(i)\n",
    "            s.union(set(f[i]))\n",
    "    t = list(set([d.get(i, -1) for i in list(s)]))\n",
    "    try:\n",
    "        while True:\n",
    "            t.remove(-1)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    tokens = [i[:i.index('-')] for i in t if '-' in i]\n",
    "    tokens+= [i for i in t if '-' not in i]\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(i, pos = 'v') for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "2558718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 'address-02', 'p': 'program', 'p2': 'prototype', 'p3': 'problem', 'r': 'risk-01', 'h': 'high-02', 'e': 'ensure-01', 'f': 'feasible', 'p4': 'program', 'p5': 'possible-01', 'u': 'use-01', 'p6': 'prototype', 's': 'show-01', 'p7': 'possible-01', 'p8': 'program-01', 's2': 'software', 'c': 'company', 'a2': 'also'}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'risk use address program high show ensure possible feasible prototype problem software'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_adjlist_text('ARG1', graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3907903",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40332547d332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'curr' is not defined"
     ]
    }
   ],
   "source": [
    "curr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4772aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "def generate_adjlist_text_rest(graphs1):\n",
    "    a = [i.replace(')',\"\") for i in graphs1[0].split(\"\\n\")]\n",
    "    b = [str((len(i) - len(i.lstrip()))//6)+i.strip() for i in a][1:]\n",
    "    c = [re.split(\"  | \", j) for j in [i.replace(\"/\",\"\") for i in [i.replace(\"(\",\"\") for i in b]]]\n",
    "    c[0] = [c[0][0][0], c[0][0][1:]]\n",
    "    d = {}\n",
    "    for i in c:\n",
    "        if len(i) == 3:\n",
    "            d[i[1]] = i[2]\n",
    "\n",
    "    c = [k for k in c]\n",
    "#     print(d, end = '\\n\\n\\n')\n",
    "    f = dict([[i, []] for i in set([w[1] for w in c])])\n",
    "    for i in range(len(c)):\n",
    "        if 'ARG0' not in c[i][0] and 'ARG1' not in c[i][0]:\n",
    "            j = i - 1\n",
    "            while j >=0 :\n",
    "                if c[j][0][0] < c[i][0][0]:\n",
    "                    break\n",
    "                j-=1\n",
    "            f[c[j][1]].append(c[i][1])\n",
    "            f[c[i][1]].append(c[j][1])\n",
    "    s = set()\n",
    "    for i in f.keys():\n",
    "        if len(f[i])!=0:\n",
    "            s.add(i)\n",
    "            s.union(set(f[i]))\n",
    "    t = list(set([d.get(i, -1) for i in list(s)]))\n",
    "    try:\n",
    "        while True:\n",
    "            t.remove(-1)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    tokens = [i[:i.index('-')] for i in t if '-' in i]\n",
    "    tokens+= [i for i in t if '-' not in i]\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(i, pos = 'v') for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2da3a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_adjlist(sub, graphs1):\n",
    "    a = [i.replace(')',\"\") for i in graphs1[0].split(\"\\n\")]\n",
    "    b = [str((len(i) - len(i.lstrip()))//6)+i.strip() for i in a][1:]\n",
    "    c = [re.split(\"  | \", j) for j in [i.replace(\"/\",\"\") for i in [i.replace(\"(\",\"\") for i in b]]]\n",
    "    c[0] = [c[0][0][0], c[0][0][1:]]\n",
    "    c = [k[:2] for k in c ]\n",
    "#     print(c, end = '\\n\\n\\n')\n",
    "    f = dict([[i, []] for i in set([w[1] for w in c])])\n",
    "    for i in range(len(c)):\n",
    "        if sub in c[i][0]:\n",
    "            j = i - 1\n",
    "            while j >=0 :\n",
    "                if c[j][0][0] < c[i][0][0]:\n",
    "                    break\n",
    "                j-=1\n",
    "            f[c[j][1]].append(c[i][1])\n",
    "            f[c[i][1]].append(c[j][1])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b835415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_adjlist_compliment(graphs1):\n",
    "    a = [i.replace(')',\"\") for i in graphs1[0].split(\"\\n\")]\n",
    "    b = [str((len(i) - len(i.lstrip()))//6)+i.strip() for i in a][1:]\n",
    "    c = [re.split(\"  | \", j) for j in [i.replace(\"/\",\"\") for i in [i.replace(\"(\",\"\") for i in b]]]\n",
    "    c[0] = [c[0][0][0], c[0][0][1:]]\n",
    "    c = [k[:2] for k in c ]\n",
    "#     print(c, end = '\\n\\n\\n')\n",
    "    f = dict([[i, []] for i in set([w[1] for w in c])])\n",
    "    for i in range(len(c)):\n",
    "        if 'ARG0' not in c[i][0] and 'ARG1' not in c[i][0]:\n",
    "            j = i - 1\n",
    "            while j >=0 :\n",
    "                if c[j][0][0] < c[i][0][0]:\n",
    "                    break\n",
    "                j-=1\n",
    "            f[c[j][1]].append(c[i][1])\n",
    "            f[c[i][1]].append(c[j][1])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5da75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_adjlist_default(graphs1):\n",
    "#     print(graphs1, end = '\\n\\n\\n')\n",
    "    a = [i.replace(')',\"\") for i in graphs1[0].split(\"\\n\")]\n",
    "#     print(a, end = '\\n\\n\\n')\n",
    "    b = [str((len(i) - len(i.lstrip()))//6)+i.strip() for i in a][1:]\n",
    "#     print(b, end = '\\n\\n\\n')\n",
    "    c = [re.split(\"  | \", j) for j in [i.replace(\"/\",\"\") for i in [i.replace(\"(\",\"\") for i in b]]]\n",
    "#     print(c, end = '\\n\\n\\n')\n",
    "#     c = [k for k in c if len(k)==2 ]\n",
    "    \n",
    "#     print(c, end = '\\n\\n\\n')\n",
    "#     e = [i.split(\" \") for i in [i.replace(\"(\",\"\") for i in[i.replace(\"/\",\"\") for i in b]]]\n",
    "    c[0] = [c[0][0][0], c[0][0][1:]]\n",
    "    c = [k[:2] for k in c ]\n",
    "#     d = dict(c)\n",
    "    \n",
    "#     print(f, end = '\\n\\n\\n')\n",
    "#     print(c, end = '\\n\\n\\n')\n",
    "    f = dict([[i, []] for i in set([w[1] for w in c])])\n",
    "    for i in range(len(c)):\n",
    "        j = i - 1\n",
    "        while j >=0 :\n",
    "            if c[j][0][0] < c[i][0][0]:\n",
    "                break\n",
    "            j-=1\n",
    "#         print(c[j][1])\n",
    "#         print(c[i][1])\n",
    "        f[c[j][1]].append(c[i][1])\n",
    "#         print(c[i][1] in f.keys(), end = '\\n\\n\\n')\n",
    "        f[c[i][1]].append(c[j][1])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b9d8474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m': [],\n",
       " 'a': [],\n",
       " 'c': ['s'],\n",
       " 's': ['c'],\n",
       " 'a2': [],\n",
       " 't': [],\n",
       " 's2': [],\n",
       " 'ii': [],\n",
       " 't2': [],\n",
       " 'o': [],\n",
       " 'p': ['ii2', 'p2'],\n",
       " 'ii2': ['p', 'p3'],\n",
       " 'p2': ['p'],\n",
       " 'p3': ['ii2'],\n",
       " 'a3': []}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_adjlist('ARG1',[curr.iloc[204]['student_amr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9914747d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varishgrover/.pyenv/versions/3.6.5/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from karateclub.dataset import GraphSetReader\n",
    "from karateclub import FeatherGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84faec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(g):\n",
    "    \n",
    "    \"\"\"Generate embeddings for a given AMR sub-graph g.\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    x = {}\n",
    "    count  = 0\n",
    "    for i in g.keys():\n",
    "        x[i] = count\n",
    "        count+=1\n",
    "\n",
    "    a = {}\n",
    "    for i in g.keys():\n",
    "        if len(g[i])>0:\n",
    "            a[x[i]] = []\n",
    "            for j in g[i]:\n",
    "                a[x[i]].append(x[j])\n",
    "\n",
    "    G.add_nodes_from(list(x.values()))\n",
    "\n",
    "    edges = []\n",
    "    for i in a.keys():\n",
    "        if len(a[i])!=0:\n",
    "            for j in a[i]:\n",
    "                edges.append((i, j))\n",
    "\n",
    "    G.add_edges_from(edges)\n",
    "#     print(G.edges)\n",
    "#     print(G.nodes)\n",
    "    model = FeatherGraph()\n",
    "    model.fit([G])\n",
    "    X = model.get_embedding()\n",
    "    return X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4b606835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_multiple(r):\n",
    "    \n",
    "    \"\"\"Generate embeddings for a given AMR sub-graph g.\"\"\"\n",
    "    res = []\n",
    "    for g in r:\n",
    "        G = nx.Graph()\n",
    "\n",
    "        x = {}\n",
    "        count  = 0\n",
    "        for i in g.keys():\n",
    "            x[i] = count\n",
    "            count+=1\n",
    "\n",
    "        a = {}\n",
    "        for i in g.keys():\n",
    "            if len(g[i])>0:\n",
    "                a[x[i]] = []\n",
    "                for j in g[i]:\n",
    "                    a[x[i]].append(x[j])\n",
    "\n",
    "        G.add_nodes_from(list(x.values()))\n",
    "\n",
    "        edges = []\n",
    "        for i in a.keys():\n",
    "            if len(a[i])!=0:\n",
    "                for j in a[i]:\n",
    "                    edges.append((i, j))\n",
    "\n",
    "        G.add_edges_from(edges)\n",
    "        res.append(G)\n",
    "#     print(G.edges)\n",
    "#     print(G.nodes)\n",
    "    model = FeatherGraph()\n",
    "    model.fit(res)\n",
    "    X = model.get_embedding()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c15f6b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varishgrover/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "graphs = stog.parse_sents([\"To simulate the behaviour of portions of the desired software product.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "88277abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg1_model = generate_adjlist('ARG0', graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "15bf2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = stog.parse_sents([\"To find problem and errors in a program before it is finalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "77bac29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg1_student = generate_adjlist('ARG0', graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0cc18588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.99969827e-01,  4.96100881e-01,  4.85793730e-01,  4.69182134e-01,\n",
       "        4.46481674e-01,  4.17986946e-01,  3.84067745e-01,  3.45164261e-01,\n",
       "        3.01781367e-01,  2.54482072e-01,  2.03880206e-01,  1.50632461e-01,\n",
       "        9.54298661e-02,  3.89888193e-02, -1.79582095e-02, -7.46721835e-02,\n",
       "       -1.30417091e-01, -1.84469495e-01, -2.36127925e-01, -2.84721979e-01,\n",
       "       -3.29621019e-01, -3.70242366e-01, -4.06058849e-01, -4.36605657e-01,\n",
       "       -4.61486365e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "        5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "        5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "        5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "        5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "        5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "        5.00000000e-01,  5.00000000e-01,  5.49295095e-03,  6.23210687e-02,\n",
       "        1.18340408e-01,  1.72823970e-01,  2.25064690e-01,  2.74384607e-01,\n",
       "        3.20143666e-01,  3.61748024e-01,  3.98657756e-01,  4.30393861e-01,\n",
       "        4.56544479e-01,  4.76770240e-01,  4.90808660e-01,  4.98477554e-01,\n",
       "        4.99677399e-01,  4.94392622e-01,  4.82691809e-01,  4.64726807e-01,\n",
       "        4.40730760e-01,  4.11015079e-01,  3.75965402e-01,  3.36036591e-01,\n",
       "        2.91746827e-01,  2.43670885e-01,  1.92432676e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        2.49984913e-01,  2.48050441e-01,  2.42896865e-01,  2.34591067e-01,\n",
       "        2.23240837e-01,  2.08993473e-01,  1.92033873e-01,  1.72582130e-01,\n",
       "        1.50890684e-01,  1.27241036e-01,  1.01940103e-01,  7.53162305e-02,\n",
       "        4.77149331e-02,  1.94944096e-02, -8.97910473e-03, -3.73360917e-02,\n",
       "       -6.52085453e-02, -9.22347475e-02, -1.18063963e-01, -1.42360989e-01,\n",
       "       -1.64810510e-01, -1.85121183e-01, -2.03029425e-01, -2.18302829e-01,\n",
       "       -2.30743183e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "        2.50000000e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "        2.50000000e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "        2.50000000e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "        2.50000000e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "        2.50000000e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "        2.50000000e-01,  2.50000000e-01,  2.74647547e-03,  3.11605344e-02,\n",
       "        5.91702039e-02,  8.64119852e-02,  1.12532345e-01,  1.37192304e-01,\n",
       "        1.60071833e-01,  1.80874012e-01,  1.99328878e-01,  2.15196930e-01,\n",
       "        2.28272240e-01,  2.38385120e-01,  2.45404330e-01,  2.49238777e-01,\n",
       "        2.49838699e-01,  2.47196311e-01,  2.41345904e-01,  2.32363404e-01,\n",
       "        2.20365380e-01,  2.05507539e-01,  1.87982701e-01,  1.68018296e-01,\n",
       "        1.45873413e-01,  1.21835442e-01,  9.62163378e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.24992457e-01,  1.24025220e-01,  1.21448432e-01,  1.17295534e-01,\n",
       "        1.11620418e-01,  1.04496737e-01,  9.60169363e-02,  8.62910652e-02,\n",
       "        7.54453418e-02,  6.36205179e-02,  5.09700514e-02,  3.76581152e-02,\n",
       "        2.38574665e-02,  9.74720482e-03, -4.48955237e-03, -1.86680459e-02,\n",
       "       -3.26042727e-02, -4.61173738e-02, -5.90319814e-02, -7.11804946e-02,\n",
       "       -8.24052549e-02, -9.25605914e-02, -1.01514712e-01, -1.09151414e-01,\n",
       "       -1.15371591e-01,  1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "        1.25000000e-01,  1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "        1.25000000e-01,  1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "        1.25000000e-01,  1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "        1.25000000e-01,  1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "        1.25000000e-01,  1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "        1.25000000e-01,  1.25000000e-01,  1.37323774e-03,  1.55802672e-02,\n",
       "        2.95851019e-02,  4.32059926e-02,  5.62661726e-02,  6.85961518e-02,\n",
       "        8.00359166e-02,  9.04370061e-02,  9.96644390e-02,  1.07598465e-01,\n",
       "        1.14136120e-01,  1.19192560e-01,  1.22702165e-01,  1.24619389e-01,\n",
       "        1.24919350e-01,  1.23598156e-01,  1.20672952e-01,  1.16181702e-01,\n",
       "        1.10182690e-01,  1.02753770e-01,  9.39913505e-02,  8.40091478e-02,\n",
       "        7.29367067e-02,  6.09177212e-02,  4.81081689e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        6.24962283e-02,  6.20126102e-02,  6.07242162e-02,  5.86477668e-02,\n",
       "        5.58102092e-02,  5.22483683e-02,  4.80084681e-02,  4.31455326e-02,\n",
       "        3.77226709e-02,  3.18102590e-02,  2.54850257e-02,  1.88290576e-02,\n",
       "        1.19287333e-02,  4.87360241e-03, -2.24477618e-03, -9.33402294e-03,\n",
       "       -1.63021363e-02, -2.30586869e-02, -2.95159907e-02, -3.55902473e-02,\n",
       "       -4.12026274e-02, -4.62802957e-02, -5.07573561e-02, -5.45757072e-02,\n",
       "       -5.76857957e-02,  6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "        6.25000000e-02,  6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "        6.25000000e-02,  6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "        6.25000000e-02,  6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "        6.25000000e-02,  6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "        6.25000000e-02,  6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "        6.25000000e-02,  6.25000000e-02,  6.86618868e-04,  7.79013359e-03,\n",
       "        1.47925510e-02,  2.16029963e-02,  2.81330863e-02,  3.42980759e-02,\n",
       "        4.00179583e-02,  4.52185030e-02,  4.98322195e-02,  5.37992326e-02,\n",
       "        5.70680599e-02,  5.95962800e-02,  6.13510825e-02,  6.23096943e-02,\n",
       "        6.24596748e-02,  6.17990778e-02,  6.03364761e-02,  5.80908509e-02,\n",
       "        5.50913450e-02,  5.13768848e-02,  4.69956753e-02,  4.20045739e-02,\n",
       "        3.64683534e-02,  3.04588606e-02,  2.40540844e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.12481142e-02,  3.10063051e-02,  3.03621081e-02,  2.93238834e-02,\n",
       "        2.79051046e-02,  2.61241841e-02,  2.40042341e-02,  2.15727663e-02,\n",
       "        1.88613355e-02,  1.59051295e-02,  1.27425129e-02,  9.41452881e-03,\n",
       "        5.96436663e-03,  2.43680120e-03, -1.12238809e-03, -4.66701147e-03,\n",
       "       -8.15106817e-03, -1.15293434e-02, -1.47579953e-02, -1.77951237e-02,\n",
       "       -2.06013137e-02, -2.31401479e-02, -2.53786781e-02, -2.72878536e-02,\n",
       "       -2.88428978e-02,  3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "        3.12500000e-02,  3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "        3.12500000e-02,  3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "        3.12500000e-02,  3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "        3.12500000e-02,  3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "        3.12500000e-02,  3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "        3.12500000e-02,  3.12500000e-02,  3.43309434e-04,  3.89506680e-03,\n",
       "        7.39627548e-03,  1.08014982e-02,  1.40665431e-02,  1.71490379e-02,\n",
       "        2.00089791e-02,  2.26092515e-02,  2.49161097e-02,  2.68996163e-02,\n",
       "        2.85340300e-02,  2.97981400e-02,  3.06755412e-02,  3.11548471e-02,\n",
       "        3.12298374e-02,  3.08995389e-02,  3.01682381e-02,  2.90454255e-02,\n",
       "        2.75456725e-02,  2.56884424e-02,  2.34978376e-02,  2.10022870e-02,\n",
       "        1.82341767e-02,  1.52294303e-02,  1.20270422e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_embedding(arg1_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e56091e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.55514087e-01,  5.50198925e-01,  5.36059483e-01,\n",
       "         5.13335026e-01,  4.82409857e-01,  4.43806484e-01,\n",
       "         3.98176335e-01,  3.46288198e-01,  2.89014597e-01,\n",
       "         2.27316358e-01,  1.62225641e-01,  9.48277746e-02,\n",
       "         2.62421925e-02, -4.23971419e-02, -1.09961491e-01,\n",
       "        -1.75346905e-01, -2.37493239e-01, -2.95402281e-01,\n",
       "        -3.48154662e-01, -3.94925261e-01, -4.34996806e-01,\n",
       "        -4.67771466e-01, -4.92780214e-01, -5.09689819e-01,\n",
       "        -5.18307343e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  5.55555556e-01,\n",
       "         5.55555556e-01,  5.55555556e-01,  6.74252276e-03,\n",
       "         7.64447703e-02,  1.44887170e-01,  2.10942895e-01,\n",
       "         2.73526548e-01,  3.31612958e-01,  3.84254888e-01,\n",
       "         4.30599317e-01,  4.69902018e-01,  5.01540170e-01,\n",
       "         5.25022762e-01,  5.39998632e-01,  5.46261981e-01,\n",
       "         5.43755279e-01,  5.32569521e-01,  5.12941840e-01,\n",
       "         4.85250532e-01,  4.50007590e-01,  4.07848911e-01,\n",
       "         3.59522358e-01,  3.05873904e-01,  2.47832140e-01,\n",
       "         1.86391424e-01,  1.22593994e-01,  5.75113907e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.14790522e-01,  3.11676963e-01,\n",
       "         3.03395908e-01,  2.90092003e-01,  2.71997497e-01,\n",
       "         2.49427995e-01,  2.22776696e-01,  1.92507214e-01,\n",
       "         1.59145135e-01,  1.23268453e-01,  8.54970715e-02,\n",
       "         4.64815763e-02,  6.89147654e-03, -3.25968523e-02,\n",
       "        -7.13123041e-02, -1.08601028e-01, -1.43838038e-01,\n",
       "        -1.76438243e-01, -2.05866672e-01, -2.31647732e-01,\n",
       "        -2.53373313e-01, -2.70709603e-01, -2.83402493e-01,\n",
       "        -2.91281473e-01, -2.94261966e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.14814815e-01,  3.14814815e-01,  3.14814815e-01,\n",
       "         3.88468729e-03,  4.40386170e-02,  8.34425128e-02,\n",
       "         1.21425933e-01,  1.57343844e-01,  1.90588127e-01,\n",
       "         2.20598407e-01,  2.46871986e-01,  2.68972706e-01,\n",
       "         2.86538573e-01,  2.99288010e-01,  3.07024617e-01,\n",
       "         3.09640359e-01,  3.07117124e-01,  2.99526636e-01,\n",
       "         2.87028713e-01,  2.69867931e-01,  2.48368748e-01,\n",
       "         2.22929190e-01,  1.94013230e-01,  1.62142002e-01,\n",
       "         1.27884028e-01,  9.18446351e-02,  5.46547866e-02,\n",
       "         1.69595188e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.82084246e-01,\n",
       "         1.80223482e-01,  1.75275398e-01,  1.67328994e-01,\n",
       "         1.56527127e-01,  1.43063833e-01,  1.27180700e-01,\n",
       "         1.09162350e-01,  8.93311253e-02,  6.80410756e-02,\n",
       "         4.56713695e-02,  2.26192475e-02, -7.07341536e-04,\n",
       "        -2.38972804e-02, -4.65438578e-02, -6.82522304e-02,\n",
       "        -8.86466316e-02, -1.07377190e-01, -1.24126230e-01,\n",
       "        -1.38613933e-01, -1.50603263e-01, -1.59904048e-01,\n",
       "        -1.66376170e-01, -1.69931779e-01, -1.70536512e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  1.82098765e-01,  1.82098765e-01,\n",
       "         1.82098765e-01,  2.28462759e-03,  2.58967964e-02,\n",
       "         4.90538747e-02,  7.13492906e-02,  9.23923021e-02,\n",
       "         1.11815162e-01,  1.29279845e-01,  1.44484212e-01,\n",
       "         1.57167484e-01,  1.67114945e-01,  1.74161758e-01,\n",
       "         1.78195842e-01,  1.79159758e-01,  1.77051552e-01,\n",
       "         1.71924569e-01,  1.63886219e-01,  1.53095743e-01,\n",
       "         1.39761009e-01,  1.24134418e-01,  1.06507982e-01,\n",
       "         8.72077015e-02,  6.65873191e-02,  4.50215997e-02,\n",
       "         2.28992530e-02,  6.15641646e-04,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.07501447e-01,  1.06368408e-01,  1.03355995e-01,\n",
       "         9.85198258e-02,  9.19491493e-02,  8.37651403e-02,\n",
       "         7.41185851e-02,  6.31870041e-02,  5.11712677e-02,\n",
       "         3.82917706e-02,  2.47842406e-02,  1.08952633e-02,\n",
       "        -3.12239064e-03, -1.70145430e-02, -3.05303994e-02,\n",
       "        -4.34272596e-02, -5.54750575e-02, -6.64606408e-02,\n",
       "        -7.61917109e-02, -8.45003451e-02, -9.12460358e-02,\n",
       "        -9.63181888e-02, -9.96380344e-02, -1.01159918e-01,\n",
       "        -1.00871943e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.07510288e-01,\n",
       "         1.07510288e-01,  1.07510288e-01,  1.37050309e-03,\n",
       "         1.55333901e-02,  2.94153496e-02,  4.27655279e-02,\n",
       "         5.53430711e-02,  6.69216467e-02,  7.72936840e-02,\n",
       "         8.62742515e-02,  9.37044962e-02,  9.94545785e-02,\n",
       "         1.03426047e-01,  1.05553611e-01,  1.05806265e-01,\n",
       "         1.04187769e-01,  1.00736451e-01,  9.55243507e-02,\n",
       "         8.86557226e-02,  8.02649283e-02,  7.05137573e-02,\n",
       "         5.95882359e-02,  4.76949843e-02,  3.50571963e-02,\n",
       "         2.19103212e-02,  8.49753299e-03, -4.93492433e-03,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  6.47236058e-02,  6.40219820e-02,\n",
       "         6.21568615e-02,  5.91634653e-02,  5.50982985e-02,\n",
       "         5.00380526e-02,  4.40781158e-02,  3.73307214e-02,\n",
       "         2.99227706e-02,  2.19933739e-02,  1.36911576e-02,\n",
       "         5.17139134e-03, -3.40700857e-03, -1.18845401e-02,\n",
       "        -2.01041800e-02, -2.79143928e-02, -3.51720232e-02,\n",
       "        -4.17450175e-02, -4.75149194e-02, -5.23790915e-02,\n",
       "        -5.62526209e-02, -5.90698708e-02, -6.07856501e-02,\n",
       "        -6.13759775e-02, -6.08384289e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         6.47290809e-02,  6.47290809e-02,  6.47290809e-02,\n",
       "         8.37377739e-04,  9.49002302e-03,  1.79666163e-02,\n",
       "         2.61100190e-02,  3.37694823e-02,  4.08035338e-02,\n",
       "         4.70826828e-02,  5.24918896e-02,  5.69327509e-02,\n",
       "         6.03253598e-02,  6.26098028e-02,  6.37472648e-02,\n",
       "         6.37207229e-02,  6.25352135e-02,  6.02176703e-02,\n",
       "         5.68163363e-02,  5.23997622e-02,  4.70554132e-02,\n",
       "         4.08879110e-02,  3.40169479e-02,  2.65749145e-02,\n",
       "         1.87042893e-02,  1.05548416e-02,  2.28070415e-03,\n",
       "        -5.96262560e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00],\n",
       "       [ 4.99969827e-01,  4.96100881e-01,  4.85793730e-01,\n",
       "         4.69182134e-01,  4.46481674e-01,  4.17986946e-01,\n",
       "         3.84067745e-01,  3.45164261e-01,  3.01781367e-01,\n",
       "         2.54482072e-01,  2.03880206e-01,  1.50632461e-01,\n",
       "         9.54298661e-02,  3.89888193e-02, -1.79582095e-02,\n",
       "        -7.46721835e-02, -1.30417091e-01, -1.84469495e-01,\n",
       "        -2.36127925e-01, -2.84721979e-01, -3.29621019e-01,\n",
       "        -3.70242366e-01, -4.06058849e-01, -4.36605657e-01,\n",
       "        -4.61486365e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n",
       "         5.00000000e-01,  5.00000000e-01,  5.49295095e-03,\n",
       "         6.23210687e-02,  1.18340408e-01,  1.72823970e-01,\n",
       "         2.25064690e-01,  2.74384607e-01,  3.20143666e-01,\n",
       "         3.61748024e-01,  3.98657756e-01,  4.30393861e-01,\n",
       "         4.56544479e-01,  4.76770240e-01,  4.90808660e-01,\n",
       "         4.98477554e-01,  4.99677399e-01,  4.94392622e-01,\n",
       "         4.82691809e-01,  4.64726807e-01,  4.40730760e-01,\n",
       "         4.11015079e-01,  3.75965402e-01,  3.36036591e-01,\n",
       "         2.91746827e-01,  2.43670885e-01,  1.92432676e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.49984913e-01,  2.48050441e-01,\n",
       "         2.42896865e-01,  2.34591067e-01,  2.23240837e-01,\n",
       "         2.08993473e-01,  1.92033873e-01,  1.72582130e-01,\n",
       "         1.50890684e-01,  1.27241036e-01,  1.01940103e-01,\n",
       "         7.53162305e-02,  4.77149331e-02,  1.94944096e-02,\n",
       "        -8.97910473e-03, -3.73360917e-02, -6.52085453e-02,\n",
       "        -9.22347475e-02, -1.18063963e-01, -1.42360989e-01,\n",
       "        -1.64810510e-01, -1.85121183e-01, -2.03029425e-01,\n",
       "        -2.18302829e-01, -2.30743183e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.50000000e-01,  2.50000000e-01,  2.50000000e-01,\n",
       "         2.74647547e-03,  3.11605344e-02,  5.91702039e-02,\n",
       "         8.64119852e-02,  1.12532345e-01,  1.37192304e-01,\n",
       "         1.60071833e-01,  1.80874012e-01,  1.99328878e-01,\n",
       "         2.15196930e-01,  2.28272240e-01,  2.38385120e-01,\n",
       "         2.45404330e-01,  2.49238777e-01,  2.49838699e-01,\n",
       "         2.47196311e-01,  2.41345904e-01,  2.32363404e-01,\n",
       "         2.20365380e-01,  2.05507539e-01,  1.87982701e-01,\n",
       "         1.68018296e-01,  1.45873413e-01,  1.21835442e-01,\n",
       "         9.62163378e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.24992457e-01,\n",
       "         1.24025220e-01,  1.21448432e-01,  1.17295534e-01,\n",
       "         1.11620418e-01,  1.04496737e-01,  9.60169363e-02,\n",
       "         8.62910652e-02,  7.54453418e-02,  6.36205179e-02,\n",
       "         5.09700514e-02,  3.76581152e-02,  2.38574665e-02,\n",
       "         9.74720482e-03, -4.48955237e-03, -1.86680459e-02,\n",
       "        -3.26042727e-02, -4.61173738e-02, -5.90319814e-02,\n",
       "        -7.11804946e-02, -8.24052549e-02, -9.25605914e-02,\n",
       "        -1.01514712e-01, -1.09151414e-01, -1.15371591e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "         1.25000000e-01,  1.37323774e-03,  1.55802672e-02,\n",
       "         2.95851019e-02,  4.32059926e-02,  5.62661726e-02,\n",
       "         6.85961518e-02,  8.00359166e-02,  9.04370061e-02,\n",
       "         9.96644390e-02,  1.07598465e-01,  1.14136120e-01,\n",
       "         1.19192560e-01,  1.22702165e-01,  1.24619389e-01,\n",
       "         1.24919350e-01,  1.23598156e-01,  1.20672952e-01,\n",
       "         1.16181702e-01,  1.10182690e-01,  1.02753770e-01,\n",
       "         9.39913505e-02,  8.40091478e-02,  7.29367067e-02,\n",
       "         6.09177212e-02,  4.81081689e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         6.24962283e-02,  6.20126102e-02,  6.07242162e-02,\n",
       "         5.86477668e-02,  5.58102092e-02,  5.22483683e-02,\n",
       "         4.80084681e-02,  4.31455326e-02,  3.77226709e-02,\n",
       "         3.18102590e-02,  2.54850257e-02,  1.88290576e-02,\n",
       "         1.19287333e-02,  4.87360241e-03, -2.24477618e-03,\n",
       "        -9.33402294e-03, -1.63021363e-02, -2.30586869e-02,\n",
       "        -2.95159907e-02, -3.55902473e-02, -4.12026274e-02,\n",
       "        -4.62802957e-02, -5.07573561e-02, -5.45757072e-02,\n",
       "        -5.76857957e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.25000000e-02,\n",
       "         6.25000000e-02,  6.25000000e-02,  6.86618868e-04,\n",
       "         7.79013359e-03,  1.47925510e-02,  2.16029963e-02,\n",
       "         2.81330863e-02,  3.42980759e-02,  4.00179583e-02,\n",
       "         4.52185030e-02,  4.98322195e-02,  5.37992326e-02,\n",
       "         5.70680599e-02,  5.95962800e-02,  6.13510825e-02,\n",
       "         6.23096943e-02,  6.24596748e-02,  6.17990778e-02,\n",
       "         6.03364761e-02,  5.80908509e-02,  5.50913450e-02,\n",
       "         5.13768848e-02,  4.69956753e-02,  4.20045739e-02,\n",
       "         3.64683534e-02,  3.04588606e-02,  2.40540844e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.12481142e-02,  3.10063051e-02,\n",
       "         3.03621081e-02,  2.93238834e-02,  2.79051046e-02,\n",
       "         2.61241841e-02,  2.40042341e-02,  2.15727663e-02,\n",
       "         1.88613355e-02,  1.59051295e-02,  1.27425129e-02,\n",
       "         9.41452881e-03,  5.96436663e-03,  2.43680120e-03,\n",
       "        -1.12238809e-03, -4.66701147e-03, -8.15106817e-03,\n",
       "        -1.15293434e-02, -1.47579953e-02, -1.77951237e-02,\n",
       "        -2.06013137e-02, -2.31401479e-02, -2.53786781e-02,\n",
       "        -2.72878536e-02, -2.88428978e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.12500000e-02,  3.12500000e-02,  3.12500000e-02,\n",
       "         3.43309434e-04,  3.89506680e-03,  7.39627548e-03,\n",
       "         1.08014982e-02,  1.40665431e-02,  1.71490379e-02,\n",
       "         2.00089791e-02,  2.26092515e-02,  2.49161097e-02,\n",
       "         2.68996163e-02,  2.85340300e-02,  2.97981400e-02,\n",
       "         3.06755412e-02,  3.11548471e-02,  3.12298374e-02,\n",
       "         3.08995389e-02,  3.01682381e-02,  2.90454255e-02,\n",
       "         2.75456725e-02,  2.56884424e-02,  2.34978376e-02,\n",
       "         2.10022870e-02,  1.82341767e-02,  1.52294303e-02,\n",
       "         1.20270422e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_embedding_multiple([arg1_model, arg1_student])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8dec0752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.55551409,  0.55019893,  0.53605948,  0.51333503,  0.48240986,\n",
       "        0.44380648,  0.39817633,  0.3462882 ,  0.2890146 ,  0.22731636,\n",
       "        0.16222564,  0.09482777,  0.02624219, -0.04239714, -0.10996149,\n",
       "       -0.17534691, -0.23749324, -0.29540228, -0.34815466, -0.39492526,\n",
       "       -0.43499681, -0.46777147, -0.49278021, -0.50968982, -0.51830734,\n",
       "        0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556,\n",
       "        0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556,\n",
       "        0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556,\n",
       "        0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556,\n",
       "        0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556,\n",
       "        0.00674252,  0.07644477,  0.14488717,  0.2109429 ,  0.27352655,\n",
       "        0.33161296,  0.38425489,  0.43059932,  0.46990202,  0.50154017,\n",
       "        0.52502276,  0.53999863,  0.54626198,  0.54375528,  0.53256952,\n",
       "        0.51294184,  0.48525053,  0.45000759,  0.40784891,  0.35952236,\n",
       "        0.3058739 ,  0.24783214,  0.18639142,  0.12259399,  0.05751139,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.31479052,  0.31167696,  0.30339591,  0.290092  ,  0.2719975 ,\n",
       "        0.249428  ,  0.2227767 ,  0.19250721,  0.15914514,  0.12326845,\n",
       "        0.08549707,  0.04648158,  0.00689148, -0.03259685, -0.0713123 ,\n",
       "       -0.10860103, -0.14383804, -0.17643824, -0.20586667, -0.23164773,\n",
       "       -0.25337331, -0.2707096 , -0.28340249, -0.29128147, -0.29426197,\n",
       "        0.31481481,  0.31481481,  0.31481481,  0.31481481,  0.31481481,\n",
       "        0.31481481,  0.31481481,  0.31481481,  0.31481481,  0.31481481,\n",
       "        0.31481481,  0.31481481,  0.31481481,  0.31481481,  0.31481481,\n",
       "        0.31481481,  0.31481481,  0.31481481,  0.31481481,  0.31481481,\n",
       "        0.31481481,  0.31481481,  0.31481481,  0.31481481,  0.31481481,\n",
       "        0.00388469,  0.04403862,  0.08344251,  0.12142593,  0.15734384,\n",
       "        0.19058813,  0.22059841,  0.24687199,  0.26897271,  0.28653857,\n",
       "        0.29928801,  0.30702462,  0.30964036,  0.30711712,  0.29952664,\n",
       "        0.28702871,  0.26986793,  0.24836875,  0.22292919,  0.19401323,\n",
       "        0.162142  ,  0.12788403,  0.09184464,  0.05465479,  0.01695952,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.18208425,  0.18022348,  0.1752754 ,  0.16732899,  0.15652713,\n",
       "        0.14306383,  0.1271807 ,  0.10916235,  0.08933113,  0.06804108,\n",
       "        0.04567137,  0.02261925, -0.00070734, -0.02389728, -0.04654386,\n",
       "       -0.06825223, -0.08864663, -0.10737719, -0.12412623, -0.13861393,\n",
       "       -0.15060326, -0.15990405, -0.16637617, -0.16993178, -0.17053651,\n",
       "        0.18209877,  0.18209877,  0.18209877,  0.18209877,  0.18209877,\n",
       "        0.18209877,  0.18209877,  0.18209877,  0.18209877,  0.18209877,\n",
       "        0.18209877,  0.18209877,  0.18209877,  0.18209877,  0.18209877,\n",
       "        0.18209877,  0.18209877,  0.18209877,  0.18209877,  0.18209877,\n",
       "        0.18209877,  0.18209877,  0.18209877,  0.18209877,  0.18209877,\n",
       "        0.00228463,  0.0258968 ,  0.04905387,  0.07134929,  0.0923923 ,\n",
       "        0.11181516,  0.12927985,  0.14448421,  0.15716748,  0.16711495,\n",
       "        0.17416176,  0.17819584,  0.17915976,  0.17705155,  0.17192457,\n",
       "        0.16388622,  0.15309574,  0.13976101,  0.12413442,  0.10650798,\n",
       "        0.0872077 ,  0.06658732,  0.0450216 ,  0.02289925,  0.00061564,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.10750145,  0.10636841,  0.103356  ,  0.09851983,  0.09194915,\n",
       "        0.08376514,  0.07411859,  0.063187  ,  0.05117127,  0.03829177,\n",
       "        0.02478424,  0.01089526, -0.00312239, -0.01701454, -0.0305304 ,\n",
       "       -0.04342726, -0.05547506, -0.06646064, -0.07619171, -0.08450035,\n",
       "       -0.09124604, -0.09631819, -0.09963803, -0.10115992, -0.10087194,\n",
       "        0.10751029,  0.10751029,  0.10751029,  0.10751029,  0.10751029,\n",
       "        0.10751029,  0.10751029,  0.10751029,  0.10751029,  0.10751029,\n",
       "        0.10751029,  0.10751029,  0.10751029,  0.10751029,  0.10751029,\n",
       "        0.10751029,  0.10751029,  0.10751029,  0.10751029,  0.10751029,\n",
       "        0.10751029,  0.10751029,  0.10751029,  0.10751029,  0.10751029,\n",
       "        0.0013705 ,  0.01553339,  0.02941535,  0.04276553,  0.05534307,\n",
       "        0.06692165,  0.07729368,  0.08627425,  0.0937045 ,  0.09945458,\n",
       "        0.10342605,  0.10555361,  0.10580626,  0.10418777,  0.10073645,\n",
       "        0.09552435,  0.08865572,  0.08026493,  0.07051376,  0.05958824,\n",
       "        0.04769498,  0.0350572 ,  0.02191032,  0.00849753, -0.00493492,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.06472361,  0.06402198,  0.06215686,  0.05916347,  0.0550983 ,\n",
       "        0.05003805,  0.04407812,  0.03733072,  0.02992277,  0.02199337,\n",
       "        0.01369116,  0.00517139, -0.00340701, -0.01188454, -0.02010418,\n",
       "       -0.02791439, -0.03517202, -0.04174502, -0.04751492, -0.05237909,\n",
       "       -0.05625262, -0.05906987, -0.06078565, -0.06137598, -0.06083843,\n",
       "        0.06472908,  0.06472908,  0.06472908,  0.06472908,  0.06472908,\n",
       "        0.06472908,  0.06472908,  0.06472908,  0.06472908,  0.06472908,\n",
       "        0.06472908,  0.06472908,  0.06472908,  0.06472908,  0.06472908,\n",
       "        0.06472908,  0.06472908,  0.06472908,  0.06472908,  0.06472908,\n",
       "        0.06472908,  0.06472908,  0.06472908,  0.06472908,  0.06472908,\n",
       "        0.00083738,  0.00949002,  0.01796662,  0.02611002,  0.03376948,\n",
       "        0.04080353,  0.04708268,  0.05249189,  0.05693275,  0.06032536,\n",
       "        0.0626098 ,  0.06374726,  0.06372072,  0.06253521,  0.06021767,\n",
       "        0.05681634,  0.05239976,  0.04705541,  0.04088791,  0.03401695,\n",
       "        0.02657491,  0.01870429,  0.01055484,  0.0022807 , -0.00596263,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg0_model_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1a177a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.55204167,  0.54670143,  0.53249861,  0.50968251,  0.47865319,\n",
       "        0.43995385,  0.39426048,  0.34236906,  0.28518045,  0.22368343,\n",
       "        0.15893613,  0.09204622,  0.02415039, -0.04360666, -0.11009313,\n",
       "       -0.1742106 , -0.23491369, -0.29122844, -0.34226917, -0.38725339,\n",
       "       -0.42551461, -0.45651268, -0.47984162, -0.49523481, -0.50256734,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.00671968,  0.07617999,  0.14435601,  0.2101003 ,  0.27230986,\n",
       "        0.32994631,  0.38205459,  0.42778023,  0.46638441,  0.49725696,\n",
       "        0.51992665,  0.53406892,  0.53951066,  0.5362321 ,  0.52436581,\n",
       "        0.50419276,  0.47613561,  0.44074937,  0.39870973,  0.35079912,\n",
       "        0.29789105,  0.24093288,  0.18092747,  0.11891407,  0.05594886,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.31218612,  0.30904229,  0.30068321,  0.2872616 ,  0.26902241,\n",
       "        0.24629801,  0.21950159,  0.18911912,  0.15569974,  0.11984508,\n",
       "        0.08219752,  0.04342778,  0.00422194, -0.03473163, -0.07275537,\n",
       "       -0.10919526, -0.14343299, -0.17489726, -0.20307405, -0.22751566,\n",
       "       -0.24784829, -0.26377808, -0.27509548, -0.28167791, -0.28349061,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.00387608,  0.04393638,  0.08322512,  0.12105351,  0.15676047,\n",
       "        0.18972522,  0.21937887,  0.24521512,  0.26679957,  0.28377763,\n",
       "        0.29588077,  0.30293109,  0.30484404,  0.30162927,  0.29338963,\n",
       "        0.28031835,  0.26269448,  0.24087659,  0.21529512,  0.18644328,\n",
       "        0.1548669 ,  0.12115332,  0.08591967,  0.04980072,  0.01343653,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.18075776,  0.1788594 ,  0.17381326,  0.16571527,  0.15471923,\n",
       "        0.14103367,  0.12491759,  0.10667523,  0.08664984,  0.06521679,\n",
       "        0.04277598,  0.01974383, -0.00345505, -0.02639628, -0.04866405,\n",
       "       -0.06985922, -0.08960709, -0.10756444, -0.12342602, -0.13693007,\n",
       "       -0.14786295, -0.15606274, -0.16142167, -0.16388751, -0.16346367,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.00229188,  0.02597519,  0.04918302,  0.07149124,  0.09249345,\n",
       "        0.11180896,  0.12909026,  0.14402971,  0.15636549,  0.16588653,\n",
       "        0.17243641,  0.17591607,  0.17628545,  0.17356376,  0.16782867,\n",
       "        0.15921427,  0.14790785,  0.13414569,  0.11820787,  0.10041221,\n",
       "        0.08110759,  0.06066661,  0.03947794,  0.01793841, -0.00355491,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.10726604,  0.10609329,  0.10297672,  0.09797771,  0.0911946 ,\n",
       "        0.08276069,  0.07284137,  0.06163071,  0.04934739,  0.03623018,\n",
       "        0.02253293,  0.00851939, -0.00554229, -0.01938496, -0.03274797,\n",
       "       -0.04538238, -0.05705591, -0.06755757, -0.0767017 , -0.08433146,\n",
       "       -0.09032176, -0.09458139, -0.09705446, -0.09772106, -0.09659718,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.00138824,  0.01573143,  0.02977544,  0.04325359,  0.05591072,\n",
       "        0.06750834,  0.07782947,  0.08668302,  0.09390749,  0.09937425,\n",
       "        0.10298989,  0.10469799,  0.10448004,  0.10235553,  0.09838129,\n",
       "        0.09265003,  0.08528814,  0.07645281,  0.06632851,  0.05512297,\n",
       "        0.04306264,  0.0303879 ,  0.01734795,  0.00419563, -0.0088178 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.06519635,  0.06445658,  0.06249109,  0.05933978,  0.0550666 ,\n",
       "        0.04975812,  0.04352174,  0.03648334,  0.0287846 ,  0.02058   ,\n",
       "        0.01203352,  0.00331518, -0.00540258, -0.01394846, -0.02215594,\n",
       "       -0.02986666, -0.03693372, -0.04322464, -0.04862398, -0.05303564,\n",
       "       -0.05638461, -0.05861839, -0.0597078 , -0.05964729, -0.05845483,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.00086019,  0.00974635,  0.01844071,  0.02677243,  0.03457823,\n",
       "        0.04170582,  0.04801705,  0.05339078,  0.05772531,  0.06094048,\n",
       "        0.06297923,  0.06380869,  0.06342069,  0.06183187,  0.05908306,\n",
       "        0.05523835,  0.0503835 ,  0.04462399,  0.03808265,  0.03089691,\n",
       "        0.02321576,  0.01519656,  0.00700157, -0.00120543, -0.00926264,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_embedding(e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "425745e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [generate_adjlist('ARG0', [i]) for i in list(curr['student_amr'][:50])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c4df1ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.55204167,  0.54670143,  0.53249861,  0.50968251,  0.47865319,\n",
       "        0.43995385,  0.39426048,  0.34236906,  0.28518045,  0.22368343,\n",
       "        0.15893613,  0.09204622,  0.02415039, -0.04360666, -0.11009313,\n",
       "       -0.1742106 , -0.23491369, -0.29122844, -0.34226917, -0.38725339,\n",
       "       -0.42551461, -0.45651268, -0.47984162, -0.49523481, -0.50256734,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.55208333,  0.55208333,  0.55208333,  0.55208333,  0.55208333,\n",
       "        0.00671968,  0.07617999,  0.14435601,  0.2101003 ,  0.27230986,\n",
       "        0.32994631,  0.38205459,  0.42778023,  0.46638441,  0.49725696,\n",
       "        0.51992665,  0.53406892,  0.53951066,  0.5362321 ,  0.52436581,\n",
       "        0.50419276,  0.47613561,  0.44074937,  0.39870973,  0.35079912,\n",
       "        0.29789105,  0.24093288,  0.18092747,  0.11891407,  0.05594886,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.31218612,  0.30904229,  0.30068321,  0.2872616 ,  0.26902241,\n",
       "        0.24629801,  0.21950159,  0.18911912,  0.15569974,  0.11984508,\n",
       "        0.08219752,  0.04342778,  0.00422194, -0.03473163, -0.07275537,\n",
       "       -0.10919526, -0.14343299, -0.17489726, -0.20307405, -0.22751566,\n",
       "       -0.24784829, -0.26377808, -0.27509548, -0.28167791, -0.28349061,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.31221065,  0.31221065,  0.31221065,  0.31221065,  0.31221065,\n",
       "        0.00387608,  0.04393638,  0.08322512,  0.12105351,  0.15676047,\n",
       "        0.18972522,  0.21937887,  0.24521512,  0.26679957,  0.28377763,\n",
       "        0.29588077,  0.30293109,  0.30484404,  0.30162927,  0.29338963,\n",
       "        0.28031835,  0.26269448,  0.24087659,  0.21529512,  0.18644328,\n",
       "        0.1548669 ,  0.12115332,  0.08591967,  0.04980072,  0.01343653,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.18075776,  0.1788594 ,  0.17381326,  0.16571527,  0.15471923,\n",
       "        0.14103367,  0.12491759,  0.10667523,  0.08664984,  0.06521679,\n",
       "        0.04277598,  0.01974383, -0.00345505, -0.02639628, -0.04866405,\n",
       "       -0.06985922, -0.08960709, -0.10756444, -0.12342602, -0.13693007,\n",
       "       -0.14786295, -0.15606274, -0.16142167, -0.16388751, -0.16346367,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.18077257,  0.18077257,  0.18077257,  0.18077257,  0.18077257,\n",
       "        0.00229188,  0.02597519,  0.04918302,  0.07149124,  0.09249345,\n",
       "        0.11180896,  0.12909026,  0.14402971,  0.15636549,  0.16588653,\n",
       "        0.17243641,  0.17591607,  0.17628545,  0.17356376,  0.16782867,\n",
       "        0.15921427,  0.14790785,  0.13414569,  0.11820787,  0.10041221,\n",
       "        0.08110759,  0.06066661,  0.03947794,  0.01793841, -0.00355491,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.10726604,  0.10609329,  0.10297672,  0.09797771,  0.0911946 ,\n",
       "        0.08276069,  0.07284137,  0.06163071,  0.04934739,  0.03623018,\n",
       "        0.02253293,  0.00851939, -0.00554229, -0.01938496, -0.03274797,\n",
       "       -0.04538238, -0.05705591, -0.06755757, -0.0767017 , -0.08433146,\n",
       "       -0.09032176, -0.09458139, -0.09705446, -0.09772106, -0.09659718,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.10727519,  0.10727519,  0.10727519,  0.10727519,  0.10727519,\n",
       "        0.00138824,  0.01573143,  0.02977544,  0.04325359,  0.05591072,\n",
       "        0.06750834,  0.07782947,  0.08668302,  0.09390749,  0.09937425,\n",
       "        0.10298989,  0.10469799,  0.10448004,  0.10235553,  0.09838129,\n",
       "        0.09265003,  0.08528814,  0.07645281,  0.06632851,  0.05512297,\n",
       "        0.04306264,  0.0303879 ,  0.01734795,  0.00419563, -0.0088178 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.06519635,  0.06445658,  0.06249109,  0.05933978,  0.0550666 ,\n",
       "        0.04975812,  0.04352174,  0.03648334,  0.0287846 ,  0.02058   ,\n",
       "        0.01203352,  0.00331518, -0.00540258, -0.01394846, -0.02215594,\n",
       "       -0.02986666, -0.03693372, -0.04322464, -0.04862398, -0.05303564,\n",
       "       -0.05638461, -0.05861839, -0.0597078 , -0.05964729, -0.05845483,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.06520212,  0.06520212,  0.06520212,  0.06520212,  0.06520212,\n",
       "        0.00086019,  0.00974635,  0.01844071,  0.02677243,  0.03457823,\n",
       "        0.04170582,  0.04801705,  0.05339078,  0.05772531,  0.06094048,\n",
       "        0.06297923,  0.06380869,  0.06342069,  0.06183187,  0.05908306,\n",
       "        0.05523835,  0.0503835 ,  0.04462399,  0.03808265,  0.03089691,\n",
       "        0.02321576,  0.01519656,  0.00700157, -0.00120543, -0.00926264,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7a69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2cee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381ae30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bd4e9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = generate_embedding_multiple(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91fac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7502c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80641503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "84c7ac13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>desired_answer</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>score_me</th>\n",
       "      <th>score_other</th>\n",
       "      <th>score_avg</th>\n",
       "      <th>student_amr</th>\n",
       "      <th>model_amr</th>\n",
       "      <th>model_amr_ARG1</th>\n",
       "      <th>...</th>\n",
       "      <th>model_amr_rest_text</th>\n",
       "      <th>student_amr_rest_text</th>\n",
       "      <th>model_amr_ARG0_text</th>\n",
       "      <th>student_amr_ARG0_text</th>\n",
       "      <th>model_amr_ARG1_text</th>\n",
       "      <th>student_amr_ARG1_text</th>\n",
       "      <th>similarity_default</th>\n",
       "      <th>similarity_ARG1</th>\n",
       "      <th>similarity_ARG0</th>\n",
       "      <th>similarity_rest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>11.7</td>\n",
       "      <td>What is the difference between an array that i...</td>\n",
       "      <td>The static arrays are intialized only once whe...</td>\n",
       "      <td>by defining static array it doese not lose sco...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.25</td>\n",
       "      <td># ::snt by defining static array it doese not ...</td>\n",
       "      <td># ::snt The static arrays are intialized only ...</td>\n",
       "      <td>[0.5951885584144773, 0.5888403851804219, 0.571...</td>\n",
       "      <td>...</td>\n",
       "      <td>call array only static</td>\n",
       "      <td>local make define lose scalable have function ...</td>\n",
       "      <td></td>\n",
       "      <td>define make lose it array</td>\n",
       "      <td>call function array</td>\n",
       "      <td>local make define scalable have lose function ...</td>\n",
       "      <td>0.663047</td>\n",
       "      <td>0.466948</td>\n",
       "      <td>0.113963</td>\n",
       "      <td>0.563441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>4.1</td>\n",
       "      <td>What are the two different ways of specifying ...</td>\n",
       "      <td>In the array declaration, or by using an initi...</td>\n",
       "      <td>1-Initializing an array in a declaration with ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td># ::snt 1-Initializing an array in a declarati...</td>\n",
       "      <td># ::snt In the array declaration, or by using ...</td>\n",
       "      <td>[0.6110583470536463, 0.6042969691664344, 0.586...</td>\n",
       "      <td>...</td>\n",
       "      <td>declare use initialize list</td>\n",
       "      <td>specify initialize declare constant variable l...</td>\n",
       "      <td></td>\n",
       "      <td>initialize list</td>\n",
       "      <td>declare use list array</td>\n",
       "      <td>specify initialize size array</td>\n",
       "      <td>0.783523</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.151953</td>\n",
       "      <td>0.797275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>10.7</td>\n",
       "      <td>How many comparisons does it take to find an e...</td>\n",
       "      <td>The height of the tree (or log of the number o...</td>\n",
       "      <td>Log(n)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td># ::snt Log(n)\\n(l / log-01\\n      :mod (s / s...</td>\n",
       "      <td># ::snt The height of the tree (or log of the ...</td>\n",
       "      <td>[0.5555140868165546, 0.5501989252153908, 0.536...</td>\n",
       "      <td>...</td>\n",
       "      <td>high log number tree element</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>high tree</td>\n",
       "      <td></td>\n",
       "      <td>0.526824</td>\n",
       "      <td>0.328337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>3.1</td>\n",
       "      <td>What does a function signature include?</td>\n",
       "      <td>The name of the function and the types of the ...</td>\n",
       "      <td>The portion of a function prototype that inclu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td># ::snt The portion of a function prototype th...</td>\n",
       "      <td># ::snt The name of the function and the types...</td>\n",
       "      <td>[0.5555140868165546, 0.5501989252153908, 0.536...</td>\n",
       "      <td>...</td>\n",
       "      <td>name type thing parameter</td>\n",
       "      <td>prototype include name argue type thing and</td>\n",
       "      <td></td>\n",
       "      <td>argue function</td>\n",
       "      <td>name function</td>\n",
       "      <td>prototype include name function and</td>\n",
       "      <td>0.739667</td>\n",
       "      <td>0.644631</td>\n",
       "      <td>0.265206</td>\n",
       "      <td>0.606859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>12.8</td>\n",
       "      <td>What is the Euler tour traversal of a tree?</td>\n",
       "      <td>A walk around the tree, starting with the root...</td>\n",
       "      <td>An Euler traversal is a traversal that begins ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td># ::snt An Euler traversal is a traversal that...</td>\n",
       "      <td># ::snt A walk around the tree, starting with ...</td>\n",
       "      <td>[0.5654311545815511, 0.5596598061588917, 0.544...</td>\n",
       "      <td>...</td>\n",
       "      <td>right mean start see leave around each below r...</td>\n",
       "      <td>right begin visit traverse leave travel bottom...</td>\n",
       "      <td></td>\n",
       "      <td>travel traverse person</td>\n",
       "      <td>mean start see node</td>\n",
       "      <td>visit begin traverse element</td>\n",
       "      <td>0.508547</td>\n",
       "      <td>0.441240</td>\n",
       "      <td>0.138486</td>\n",
       "      <td>0.598849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           question  \\\n",
       "188   11.7  What is the difference between an array that i...   \n",
       "1320   4.1  What are the two different ways of specifying ...   \n",
       "2221  10.7  How many comparisons does it take to find an e...   \n",
       "1087   3.1            What does a function signature include?   \n",
       "674   12.8        What is the Euler tour traversal of a tree?   \n",
       "\n",
       "                                         desired_answer  \\\n",
       "188   The static arrays are intialized only once whe...   \n",
       "1320  In the array declaration, or by using an initi...   \n",
       "2221  The height of the tree (or log of the number o...   \n",
       "1087  The name of the function and the types of the ...   \n",
       "674   A walk around the tree, starting with the root...   \n",
       "\n",
       "                                         student_answer  score_me  \\\n",
       "188   by defining static array it doese not lose sco...       4.0   \n",
       "1320  1-Initializing an array in a declaration with ...       5.0   \n",
       "2221                                             Log(n)       5.0   \n",
       "1087  The portion of a function prototype that inclu...       5.0   \n",
       "674   An Euler traversal is a traversal that begins ...       5.0   \n",
       "\n",
       "      score_other  score_avg  \\\n",
       "188           4.5       4.25   \n",
       "1320          5.0       5.00   \n",
       "2221          5.0       5.00   \n",
       "1087          5.0       5.00   \n",
       "674           5.0       5.00   \n",
       "\n",
       "                                            student_amr  \\\n",
       "188   # ::snt by defining static array it doese not ...   \n",
       "1320  # ::snt 1-Initializing an array in a declarati...   \n",
       "2221  # ::snt Log(n)\\n(l / log-01\\n      :mod (s / s...   \n",
       "1087  # ::snt The portion of a function prototype th...   \n",
       "674   # ::snt An Euler traversal is a traversal that...   \n",
       "\n",
       "                                              model_amr  \\\n",
       "188   # ::snt The static arrays are intialized only ...   \n",
       "1320  # ::snt In the array declaration, or by using ...   \n",
       "2221  # ::snt The height of the tree (or log of the ...   \n",
       "1087  # ::snt The name of the function and the types...   \n",
       "674   # ::snt A walk around the tree, starting with ...   \n",
       "\n",
       "                                         model_amr_ARG1  ...  \\\n",
       "188   [0.5951885584144773, 0.5888403851804219, 0.571...  ...   \n",
       "1320  [0.6110583470536463, 0.6042969691664344, 0.586...  ...   \n",
       "2221  [0.5555140868165546, 0.5501989252153908, 0.536...  ...   \n",
       "1087  [0.5555140868165546, 0.5501989252153908, 0.536...  ...   \n",
       "674   [0.5654311545815511, 0.5596598061588917, 0.544...  ...   \n",
       "\n",
       "                                    model_amr_rest_text  \\\n",
       "188                              call array only static   \n",
       "1320                        declare use initialize list   \n",
       "2221                       high log number tree element   \n",
       "1087                          name type thing parameter   \n",
       "674   right mean start see leave around each below r...   \n",
       "\n",
       "                                  student_amr_rest_text model_amr_ARG0_text  \\\n",
       "188   local make define lose scalable have function ...                       \n",
       "1320  specify initialize declare constant variable l...                       \n",
       "2221                                             string                       \n",
       "1087        prototype include name argue type thing and                       \n",
       "674   right begin visit traverse leave travel bottom...                       \n",
       "\n",
       "          student_amr_ARG0_text     model_amr_ARG1_text  \\\n",
       "188   define make lose it array     call function array   \n",
       "1320            initialize list  declare use list array   \n",
       "2221                                          high tree   \n",
       "1087             argue function           name function   \n",
       "674      travel traverse person     mean start see node   \n",
       "\n",
       "                                  student_amr_ARG1_text similarity_default  \\\n",
       "188   local make define scalable have lose function ...           0.663047   \n",
       "1320                      specify initialize size array           0.783523   \n",
       "2221                                                              0.526824   \n",
       "1087                prototype include name function and           0.739667   \n",
       "674                        visit begin traverse element           0.508547   \n",
       "\n",
       "      similarity_ARG1 similarity_ARG0 similarity_rest  \n",
       "188          0.466948        0.113963        0.563441  \n",
       "1320         0.505571        0.151953        0.797275  \n",
       "2221         0.328337        1.000000        0.131471  \n",
       "1087         0.644631        0.265206        0.606859  \n",
       "674          0.441240        0.138486        0.598849  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb39ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacf820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44bc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b6c5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "#     return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c7cbec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995226264942632"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_cosine_similarity(arg1_model_embedding, arg1_student_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "706b72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.random.normal([32, 10, 8])\n",
    "lstm = tf.keras.layers.LSTM(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ba1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# i = tf.keras.Input(shape=(500,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d23007fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ARG0\n",
    "\n",
    "# lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(500))\n",
    "\n",
    "# #--------------------Classification Pipeline\n",
    "# Using LSTM layer to encode our embeddings\n",
    "# input1 = tf.keras.Input(shape=(1000,))\n",
    "# # d1 = tf.keras.layers.Dense(1000,input_shape = (1000,), activation='relu')(input1)\n",
    "# d3 = tf.keras.layers.Dense(20, activation='sigmoid')(input1)\n",
    "# d4 = tf.keras.layers.Dense(6, activation='softmax')(d3)\n",
    "# model1 = tf.keras.Model(inputs=input1, outputs=d4)\n",
    "\n",
    "# input2 = tf.keras.Input(shape=(1000,))\n",
    "# # d5 = tf.keras.layers.Dense(1000,input_shape = (1000,), activation='relu')(input2)\n",
    "# d7 = tf.keras.layers.Dense(20, activation='sigmoid')(input2)\n",
    "# d8 = tf.keras.layers.Dense(6, activation='softmax')(d7)\n",
    "# model2 = tf.keras.Model(inputs=input2, outputs=d8)\n",
    "\n",
    "# input3 = tf.keras.Input(shape=(1000,))\n",
    "# # d1 = tf.keras.layers.Dense(1000,input_shape = (1000,), activation='relu')(input1)\n",
    "# d31 = tf.keras.layers.Dense(20, activation='sigmoid')(input3)\n",
    "# d41 = tf.keras.layers.Dense(6, activation='softmax')(d31)\n",
    "# model3 = tf.keras.Model(inputs=input3, outputs=d41)\n",
    "\n",
    "# input4 = tf.keras.Input(shape=(1000,))\n",
    "# # d5 = tf.keras.layers.Dense(1000,input_shape = (1000,), activation='relu')(input2)\n",
    "# d71 = tf.keras.layers.Dense(20, activation='sigmoid')(input4)\n",
    "# d81 = tf.keras.layers.Dense(6, activation='softmax')(d71)\n",
    "# model4 = tf.keras.Model(inputs=input4, outputs=d81)\n",
    "\n",
    "\n",
    "# input5 = tf.keras.Input(shape=(1,))\n",
    "# input6 = tf.keras.Input(shape=(1,))\n",
    "# input7 = tf.keras.Input(shape=(1,))\n",
    "# input8 = tf.keras.Input(shape=(1,))\n",
    "\n",
    "# combined_text = tf.keras.layers.concatenate([input5,input6,input7,input8])\n",
    "# d100 = tf.keras.layers.Dense(6, activation='softmax')(combined_text)\n",
    "# # print(d100)\n",
    "# model5 = tf.keras.Model(inputs= [input5, input6, input7, input8], outputs = d100)\n",
    "\n",
    "# # combine the output of the two branches\n",
    "# combined = tf.keras.layers.Average()([model1.output, model2.output, model3.output, model4.output, model5.output])\n",
    "# # apply a FC layer and then a regression prediction on the\n",
    "# # combined outputs\n",
    "# # z = tf.keras.layers.Dense(3)(combined)\n",
    "# # z = tf.keras.layers.Dense(6, activation=\"softmax\")(z)\n",
    "# # our model will accept the inputs of the two branches and\n",
    "# # then output a single value\n",
    "# model = tf.keras.Model(inputs=[model1.input, model2.input, model3.input, model4.input]+model5.inputs, outputs=combined)\n",
    "\n",
    "# \"# ------------------Regression Pipeline\n",
    "# Using LSTM layer to encode our embeddings\n",
    "# ------------------Regression Pipeline\n",
    "# Using LSTM layer to encode our embeddings\n",
    "input1 = tf.keras.Input(shape=(1000,))\n",
    "d1 = tf.keras.layers.Dense(10,input_shape = (10,), activation='sigmoid', trainable = True)(input1)\n",
    "d4 = tf.keras.layers.Dense(5, activation='relu')(d1)\n",
    "\n",
    "model1 = tf.keras.Model(inputs=input1, outputs=d4)\n",
    "\n",
    "input2 = tf.keras.Input(shape=(1000,))\n",
    "d5 = tf.keras.layers.Dense(10,input_shape = (10,), activation='sigmoid')(input2)\n",
    "d8 = tf.keras.layers.Dense(5, activation='relu')(d5)\n",
    "\n",
    "model2 = tf.keras.Model(inputs=input2, outputs=d8)\n",
    "\n",
    "input3 = tf.keras.Input(shape=(1000,))\n",
    "d11 = tf.keras.layers.Dense(10,input_shape = (10,), activation='sigmoid', trainable = True)(input3)\n",
    "d41 = tf.keras.layers.Dense(5, activation='relu')(d11)\n",
    "\n",
    "model3 = tf.keras.Model(inputs=input3, outputs=d41)\n",
    "\n",
    "input4 = tf.keras.Input(shape=(1000,))\n",
    "d51 = tf.keras.layers.Dense(10,input_shape = (10,), activation='sigmoid')(input4)\n",
    "d81 = tf.keras.layers.Dense(5, activation='relu')(d51)\n",
    "\n",
    "model4 = tf.keras.Model(inputs=input4, outputs=d81)\n",
    "\n",
    "input5 = tf.keras.Input(shape=(1,))\n",
    "input6 = tf.keras.Input(shape=(1,))\n",
    "input7 = tf.keras.Input(shape=(1,))\n",
    "input8 = tf.keras.Input(shape=(1,))\n",
    "\n",
    "combined_text = tf.keras.layers.concatenate([input5,input6,input7,input8])\n",
    "d100 = tf.keras.layers.Dense(10, activation='sigmoid')(combined_text)\n",
    "model5 = tf.keras.Model(inputs= [input5, input6, input7, input8], outputs = d100)\n",
    "\n",
    "combined = tf.keras.layers.concatenate([model1.output, model2.output, model3.output, model4.output, model5.output])\n",
    "z1 = tf.keras.layers.Dense(10, activation=\"sigmoid\", trainable = True)(combined)\n",
    "z2 = tf.keras.layers.Dense(4, activation = 'relu', trainable = True)(combined_text)\n",
    "\n",
    "combined_final = tf.keras.layers.concatenate([z1, z2])\n",
    "z = tf.keras.layers.Dense(10, activation=\"linear\", trainable = True)(combined_final)\n",
    "model = tf.keras.Model(inputs=[model1.input, model2.input, model3.input, model4.input]+model5.inputs, outputs=z)\n",
    "\n",
    "# input2 = tf.keras.Input(shape=(2, 500))\n",
    "# x2 = lstm_layer(input2)\n",
    "# x2 = tf.keras.Model(inputs=input2, outputs=x2)\n",
    "\n",
    "# concat  = tf.keras.layers.concatenate([x1, x2],axis=1)\n",
    "\n",
    "# mhd = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# merged = tf.keras.layers.Lambda(function=mhd, output_shape=lambda x: x[0],name='L1_distance')([x1.output, x2.output])\n",
    "# preds = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# x = tf.keras.Model(inputs=[x1.input, x2.input], outputs=preds)\n",
    "\n",
    "# #FOR ARG1\n",
    "# lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1, dropout=0.2, recurrent_dropout=0.2))\n",
    "# input3 = tf.keras.Input(shape=(1, 500))\n",
    "# x3 = lstm_layer(input3)\n",
    "# input4 = tf.keras.Input(shape=(1, 500))\n",
    "# x4 = lstm_layer(input4)\n",
    "# merged_2 = tf.keras.layers.Lambda(function=mhd, output_shape=lambda x: x[0],name='L1_distance_2')([x3, x4])\n",
    "# preds_2 = tf.keras.layers.Dense(1, activation='sigmoid')(merged_2)\n",
    "\n",
    "# out = tf.keras.layers.Dense(1, activation='sigmoid')(merged_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7f82d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---------py toruch implementation\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Model, self).__init__()\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "51e2c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "315285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "537603c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "# ------------   -------------\n",
    "# ------------  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f0403853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_149\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_194 (InputLayer)          [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_195 (InputLayer)          [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_196 (InputLayer)          [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_197 (InputLayer)          [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_198 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_199 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_200 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_201 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_277 (Dense)               (None, 10)           10010       input_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_279 (Dense)               (None, 10)           10010       input_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_281 (Dense)               (None, 10)           10010       input_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_283 (Dense)               (None, 10)           10010       input_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 4)            0           input_198[0][0]                  \n",
      "                                                                 input_199[0][0]                  \n",
      "                                                                 input_200[0][0]                  \n",
      "                                                                 input_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_278 (Dense)               (None, 5)            55          dense_277[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_280 (Dense)               (None, 5)            55          dense_279[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_282 (Dense)               (None, 5)            55          dense_281[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_284 (Dense)               (None, 5)            55          dense_283[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_285 (Dense)               (None, 10)           50          concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 30)           0           dense_278[0][0]                  \n",
      "                                                                 dense_280[0][0]                  \n",
      "                                                                 dense_282[0][0]                  \n",
      "                                                                 dense_284[0][0]                  \n",
      "                                                                 dense_285[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_286 (Dense)               (None, 10)           310         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_287 (Dense)               (None, 4)            20          concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 14)           0           dense_286[0][0]                  \n",
      "                                                                 dense_287[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_288 (Dense)               (None, 10)           150         concatenate_77[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 40,790\n",
      "Trainable params: 40,790\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.6510 - root_mean_squared_error: 3.6475 - val_loss: 3.3264 - val_root_mean_squared_error: 3.3476\n",
      "Epoch 2/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9193 - root_mean_squared_error: 2.9183 - val_loss: 2.6284 - val_root_mean_squared_error: 2.6479\n",
      "Epoch 3/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2519 - root_mean_squared_error: 2.2501 - val_loss: 1.9360 - val_root_mean_squared_error: 1.9517\n",
      "Epoch 4/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6421 - root_mean_squared_error: 1.6410 - val_loss: 1.3852 - val_root_mean_squared_error: 1.3931\n",
      "Epoch 5/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2921 - root_mean_squared_error: 1.2893 - val_loss: 1.1103 - val_root_mean_squared_error: 1.1125\n",
      "Epoch 6/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1274 - root_mean_squared_error: 1.1253 - val_loss: 0.9805 - val_root_mean_squared_error: 0.9790\n",
      "Epoch 7/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0664 - root_mean_squared_error: 1.0682 - val_loss: 0.9653 - val_root_mean_squared_error: 0.9639\n",
      "Epoch 8/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0606 - root_mean_squared_error: 1.0606 - val_loss: 0.9417 - val_root_mean_squared_error: 0.9367\n",
      "Epoch 9/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0503 - root_mean_squared_error: 1.0477 - val_loss: 0.9314 - val_root_mean_squared_error: 0.9255\n",
      "Epoch 10/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0421 - root_mean_squared_error: 1.0410 - val_loss: 0.9236 - val_root_mean_squared_error: 0.9177\n",
      "Epoch 11/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0350 - root_mean_squared_error: 1.0316 - val_loss: 0.9142 - val_root_mean_squared_error: 0.9067\n",
      "Epoch 12/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0267 - root_mean_squared_error: 1.0234 - val_loss: 0.9063 - val_root_mean_squared_error: 0.8981\n",
      "Epoch 13/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0260 - root_mean_squared_error: 1.0268 - val_loss: 0.8985 - val_root_mean_squared_error: 0.8903\n",
      "Epoch 14/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0168 - root_mean_squared_error: 1.0152 - val_loss: 0.8935 - val_root_mean_squared_error: 0.8858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0097 - root_mean_squared_error: 1.0109 - val_loss: 0.8857 - val_root_mean_squared_error: 0.8768\n",
      "Epoch 16/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0019 - root_mean_squared_error: 1.0048 - val_loss: 0.8840 - val_root_mean_squared_error: 0.8762\n",
      "Epoch 17/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9990 - root_mean_squared_error: 0.9965 - val_loss: 0.8740 - val_root_mean_squared_error: 0.8643\n",
      "Epoch 18/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9966 - root_mean_squared_error: 0.9967 - val_loss: 0.8689 - val_root_mean_squared_error: 0.8592\n",
      "Epoch 19/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9888 - root_mean_squared_error: 0.9875 - val_loss: 0.8625 - val_root_mean_squared_error: 0.8516\n",
      "Epoch 20/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9847 - root_mean_squared_error: 0.9867 - val_loss: 0.8564 - val_root_mean_squared_error: 0.8456\n",
      "Epoch 21/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9813 - root_mean_squared_error: 0.9832 - val_loss: 0.8509 - val_root_mean_squared_error: 0.8406\n",
      "Epoch 22/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9746 - root_mean_squared_error: 0.9749 - val_loss: 0.8462 - val_root_mean_squared_error: 0.8364\n",
      "Epoch 23/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9651 - root_mean_squared_error: 0.9635 - val_loss: 0.8425 - val_root_mean_squared_error: 0.8301\n",
      "Epoch 24/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9630 - root_mean_squared_error: 0.9655 - val_loss: 0.8350 - val_root_mean_squared_error: 0.8255\n",
      "Epoch 25/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9553 - root_mean_squared_error: 0.9567 - val_loss: 0.8288 - val_root_mean_squared_error: 0.8192\n",
      "Epoch 26/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9534 - root_mean_squared_error: 0.9519 - val_loss: 0.8253 - val_root_mean_squared_error: 0.8127\n",
      "Epoch 27/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9517 - root_mean_squared_error: 0.9499 - val_loss: 0.8188 - val_root_mean_squared_error: 0.8071\n",
      "Epoch 28/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9439 - root_mean_squared_error: 0.9420 - val_loss: 0.8178 - val_root_mean_squared_error: 0.8058\n",
      "Epoch 29/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9461 - root_mean_squared_error: 0.9456 - val_loss: 0.8114 - val_root_mean_squared_error: 0.8019\n",
      "Epoch 30/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9369 - root_mean_squared_error: 0.9382 - val_loss: 0.8080 - val_root_mean_squared_error: 0.7985\n",
      "Epoch 31/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9358 - root_mean_squared_error: 0.9384 - val_loss: 0.8134 - val_root_mean_squared_error: 0.8061\n",
      "Epoch 32/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9324 - root_mean_squared_error: 0.9330 - val_loss: 0.8050 - val_root_mean_squared_error: 0.7965\n",
      "Epoch 33/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9294 - root_mean_squared_error: 0.9272 - val_loss: 0.8065 - val_root_mean_squared_error: 0.7944\n",
      "Epoch 34/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9303 - root_mean_squared_error: 0.9294 - val_loss: 0.8045 - val_root_mean_squared_error: 0.7932\n",
      "Epoch 35/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9280 - root_mean_squared_error: 0.9296 - val_loss: 0.7984 - val_root_mean_squared_error: 0.7889\n",
      "Epoch 36/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9242 - root_mean_squared_error: 0.9237 - val_loss: 0.7962 - val_root_mean_squared_error: 0.7860\n",
      "Epoch 37/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9262 - root_mean_squared_error: 0.9250 - val_loss: 0.8032 - val_root_mean_squared_error: 0.7961\n",
      "Epoch 38/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9227 - root_mean_squared_error: 0.9245 - val_loss: 0.7944 - val_root_mean_squared_error: 0.7858\n",
      "Epoch 39/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9184 - root_mean_squared_error: 0.9211 - val_loss: 0.7984 - val_root_mean_squared_error: 0.7920\n",
      "Epoch 40/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9177 - root_mean_squared_error: 0.9183 - val_loss: 0.8058 - val_root_mean_squared_error: 0.7933\n",
      "Epoch 41/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9217 - root_mean_squared_error: 0.9195 - val_loss: 0.7934 - val_root_mean_squared_error: 0.7867\n",
      "Epoch 42/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9186 - root_mean_squared_error: 0.9207 - val_loss: 0.7943 - val_root_mean_squared_error: 0.7884\n",
      "Epoch 43/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9145 - root_mean_squared_error: 0.9140 - val_loss: 0.7889 - val_root_mean_squared_error: 0.7812\n",
      "Epoch 44/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9155 - root_mean_squared_error: 0.9173 - val_loss: 0.7879 - val_root_mean_squared_error: 0.7803\n",
      "Epoch 45/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9162 - root_mean_squared_error: 0.9146 - val_loss: 0.7886 - val_root_mean_squared_error: 0.7813\n",
      "Epoch 46/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9141 - root_mean_squared_error: 0.9134 - val_loss: 0.7863 - val_root_mean_squared_error: 0.7779\n",
      "Epoch 47/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9131 - root_mean_squared_error: 0.9117 - val_loss: 0.7886 - val_root_mean_squared_error: 0.7797\n",
      "Epoch 48/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9144 - root_mean_squared_error: 0.9127 - val_loss: 0.7861 - val_root_mean_squared_error: 0.7786\n",
      "Epoch 49/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9125 - root_mean_squared_error: 0.9110 - val_loss: 0.7832 - val_root_mean_squared_error: 0.7741\n",
      "Epoch 50/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9135 - root_mean_squared_error: 0.9138 - val_loss: 0.7946 - val_root_mean_squared_error: 0.7907\n",
      "Epoch 51/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9106 - root_mean_squared_error: 0.9094 - val_loss: 0.7858 - val_root_mean_squared_error: 0.7802\n",
      "Epoch 52/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9108 - root_mean_squared_error: 0.9138 - val_loss: 0.7856 - val_root_mean_squared_error: 0.7804\n",
      "Epoch 53/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9099 - root_mean_squared_error: 0.9107 - val_loss: 0.7858 - val_root_mean_squared_error: 0.7798\n",
      "Epoch 54/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9091 - root_mean_squared_error: 0.9085 - val_loss: 0.7831 - val_root_mean_squared_error: 0.7751\n",
      "Epoch 55/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9045 - root_mean_squared_error: 0.9054 - val_loss: 0.7850 - val_root_mean_squared_error: 0.7789\n",
      "Epoch 56/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9062 - root_mean_squared_error: 0.9069 - val_loss: 0.7963 - val_root_mean_squared_error: 0.7867\n",
      "Epoch 57/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9093 - root_mean_squared_error: 0.9096 - val_loss: 0.7838 - val_root_mean_squared_error: 0.7771\n",
      "Epoch 58/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9047 - root_mean_squared_error: 0.9051 - val_loss: 0.7911 - val_root_mean_squared_error: 0.7809\n",
      "Epoch 59/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9059 - root_mean_squared_error: 0.9063 - val_loss: 0.7829 - val_root_mean_squared_error: 0.7761\n",
      "Epoch 60/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9038 - root_mean_squared_error: 0.9059 - val_loss: 0.7832 - val_root_mean_squared_error: 0.7744\n",
      "Epoch 61/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9042 - root_mean_squared_error: 0.9034 - val_loss: 0.7894 - val_root_mean_squared_error: 0.7786\n",
      "Epoch 62/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9029 - root_mean_squared_error: 0.9024 - val_loss: 0.7825 - val_root_mean_squared_error: 0.7763\n",
      "Epoch 63/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9018 - root_mean_squared_error: 0.9003 - val_loss: 0.7885 - val_root_mean_squared_error: 0.7844\n",
      "Epoch 64/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9041 - root_mean_squared_error: 0.9022 - val_loss: 0.7826 - val_root_mean_squared_error: 0.7757\n",
      "Epoch 65/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9050 - root_mean_squared_error: 0.9071 - val_loss: 0.7828 - val_root_mean_squared_error: 0.7775\n",
      "Epoch 66/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9045 - root_mean_squared_error: 0.9050 - val_loss: 0.7844 - val_root_mean_squared_error: 0.7779\n",
      "Epoch 67/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9010 - root_mean_squared_error: 0.9003 - val_loss: 0.7883 - val_root_mean_squared_error: 0.7829\n",
      "Epoch 68/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9056 - root_mean_squared_error: 0.9040 - val_loss: 0.7812 - val_root_mean_squared_error: 0.7742\n",
      "Epoch 69/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8996 - root_mean_squared_error: 0.8972 - val_loss: 0.7990 - val_root_mean_squared_error: 0.7857\n",
      "Epoch 70/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8998 - root_mean_squared_error: 0.8965 - val_loss: 0.7824 - val_root_mean_squared_error: 0.7775\n",
      "Epoch 71/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9012 - root_mean_squared_error: 0.9024 - val_loss: 0.7836 - val_root_mean_squared_error: 0.7788\n",
      "Epoch 72/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9011 - root_mean_squared_error: 0.9000 - val_loss: 0.7834 - val_root_mean_squared_error: 0.7781\n",
      "Epoch 73/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8986 - root_mean_squared_error: 0.8968 - val_loss: 0.7844 - val_root_mean_squared_error: 0.7758\n",
      "Epoch 74/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8965 - root_mean_squared_error: 0.8960 - val_loss: 0.7871 - val_root_mean_squared_error: 0.7830\n",
      "Epoch 75/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8985 - root_mean_squared_error: 0.9002 - val_loss: 0.7835 - val_root_mean_squared_error: 0.7783\n",
      "Epoch 76/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8983 - root_mean_squared_error: 0.8993 - val_loss: 0.7821 - val_root_mean_squared_error: 0.7743\n",
      "Epoch 77/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8973 - root_mean_squared_error: 0.8984 - val_loss: 0.7836 - val_root_mean_squared_error: 0.7767\n",
      "Epoch 78/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8949 - root_mean_squared_error: 0.8959 - val_loss: 0.7869 - val_root_mean_squared_error: 0.7775\n",
      "Epoch 79/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8934 - root_mean_squared_error: 0.8918 - val_loss: 0.7836 - val_root_mean_squared_error: 0.7747\n",
      "Epoch 80/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8980 - root_mean_squared_error: 0.8981 - val_loss: 0.7881 - val_root_mean_squared_error: 0.7847\n",
      "Epoch 81/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8932 - root_mean_squared_error: 0.8973 - val_loss: 0.7844 - val_root_mean_squared_error: 0.7789\n",
      "Epoch 82/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8946 - root_mean_squared_error: 0.8948 - val_loss: 0.7868 - val_root_mean_squared_error: 0.7754\n",
      "Epoch 83/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8919 - root_mean_squared_error: 0.8894 - val_loss: 0.7860 - val_root_mean_squared_error: 0.7786\n",
      "Epoch 84/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8939 - root_mean_squared_error: 0.8948 - val_loss: 0.7861 - val_root_mean_squared_error: 0.7805\n",
      "Epoch 85/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8941 - root_mean_squared_error: 0.8941 - val_loss: 0.7876 - val_root_mean_squared_error: 0.7825\n",
      "Epoch 86/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8917 - root_mean_squared_error: 0.8906 - val_loss: 0.7990 - val_root_mean_squared_error: 0.7851\n",
      "Epoch 87/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8949 - root_mean_squared_error: 0.8951 - val_loss: 0.7863 - val_root_mean_squared_error: 0.7759\n",
      "Epoch 88/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8903 - root_mean_squared_error: 0.8920 - val_loss: 0.7987 - val_root_mean_squared_error: 0.7962\n",
      "Epoch 89/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8924 - root_mean_squared_error: 0.8931 - val_loss: 0.7909 - val_root_mean_squared_error: 0.7854\n",
      "Epoch 90/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8889 - root_mean_squared_error: 0.8886 - val_loss: 0.7861 - val_root_mean_squared_error: 0.7787\n",
      "Epoch 91/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8915 - root_mean_squared_error: 0.8943 - val_loss: 0.7870 - val_root_mean_squared_error: 0.7786\n",
      "Epoch 92/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8869 - root_mean_squared_error: 0.8892 - val_loss: 0.8000 - val_root_mean_squared_error: 0.7989\n",
      "Epoch 93/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8914 - root_mean_squared_error: 0.8923 - val_loss: 0.8073 - val_root_mean_squared_error: 0.8059\n",
      "Epoch 94/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8888 - root_mean_squared_error: 0.8903 - val_loss: 0.8077 - val_root_mean_squared_error: 0.8063\n",
      "Epoch 95/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8840 - root_mean_squared_error: 0.8850 - val_loss: 0.7950 - val_root_mean_squared_error: 0.7917\n",
      "Epoch 96/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8897 - root_mean_squared_error: 0.8893 - val_loss: 0.7907 - val_root_mean_squared_error: 0.7865\n",
      "Epoch 97/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8897 - root_mean_squared_error: 0.8896 - val_loss: 0.8130 - val_root_mean_squared_error: 0.8135\n",
      "Epoch 98/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8892 - root_mean_squared_error: 0.8874 - val_loss: 0.7998 - val_root_mean_squared_error: 0.7871\n",
      "Epoch 99/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8871 - root_mean_squared_error: 0.8846 - val_loss: 0.7951 - val_root_mean_squared_error: 0.7917\n",
      "Epoch 100/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8845 - root_mean_squared_error: 0.8848 - val_loss: 0.7874 - val_root_mean_squared_error: 0.7788\n",
      "Epoch 101/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8857 - root_mean_squared_error: 0.8872 - val_loss: 0.8094 - val_root_mean_squared_error: 0.8074\n",
      "Epoch 102/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8862 - root_mean_squared_error: 0.8867 - val_loss: 0.7897 - val_root_mean_squared_error: 0.7815\n",
      "Epoch 103/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8829 - root_mean_squared_error: 0.8809 - val_loss: 0.7917 - val_root_mean_squared_error: 0.7822\n",
      "Epoch 104/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8850 - root_mean_squared_error: 0.8847 - val_loss: 0.7939 - val_root_mean_squared_error: 0.7889\n",
      "Epoch 105/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8842 - root_mean_squared_error: 0.8828 - val_loss: 0.7882 - val_root_mean_squared_error: 0.7808\n",
      "Epoch 106/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8839 - root_mean_squared_error: 0.8826 - val_loss: 0.8032 - val_root_mean_squared_error: 0.7910\n",
      "Epoch 107/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8843 - root_mean_squared_error: 0.8844 - val_loss: 0.7929 - val_root_mean_squared_error: 0.7822\n",
      "Epoch 108/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8812 - root_mean_squared_error: 0.8810 - val_loss: 0.7928 - val_root_mean_squared_error: 0.7865\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8822 - root_mean_squared_error: 0.8811 - val_loss: 0.8127 - val_root_mean_squared_error: 0.8128\n",
      "Epoch 110/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8828 - root_mean_squared_error: 0.8830 - val_loss: 0.7913 - val_root_mean_squared_error: 0.7854\n",
      "Epoch 111/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8854 - root_mean_squared_error: 0.8852 - val_loss: 0.7964 - val_root_mean_squared_error: 0.7919\n",
      "Epoch 112/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8800 - root_mean_squared_error: 0.8804 - val_loss: 0.8390 - val_root_mean_squared_error: 0.8428\n",
      "Epoch 113/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8839 - root_mean_squared_error: 0.8846 - val_loss: 0.7920 - val_root_mean_squared_error: 0.7815\n",
      "Epoch 114/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8793 - root_mean_squared_error: 0.8773 - val_loss: 0.7931 - val_root_mean_squared_error: 0.7818\n",
      "Epoch 115/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8790 - root_mean_squared_error: 0.8789 - val_loss: 0.7902 - val_root_mean_squared_error: 0.7822\n",
      "Epoch 116/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8787 - root_mean_squared_error: 0.8778 - val_loss: 0.8000 - val_root_mean_squared_error: 0.7966\n",
      "Epoch 117/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8768 - root_mean_squared_error: 0.8785 - val_loss: 0.7943 - val_root_mean_squared_error: 0.7837\n",
      "Epoch 118/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8801 - root_mean_squared_error: 0.8821 - val_loss: 0.7963 - val_root_mean_squared_error: 0.7928\n",
      "Epoch 119/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8792 - root_mean_squared_error: 0.8796 - val_loss: 0.7984 - val_root_mean_squared_error: 0.7930\n",
      "Epoch 120/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8765 - root_mean_squared_error: 0.8769 - val_loss: 0.7927 - val_root_mean_squared_error: 0.7869\n",
      "Epoch 121/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8754 - root_mean_squared_error: 0.8727 - val_loss: 0.7975 - val_root_mean_squared_error: 0.7869\n",
      "Epoch 122/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8801 - root_mean_squared_error: 0.8803 - val_loss: 0.7952 - val_root_mean_squared_error: 0.7910\n",
      "Epoch 123/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8781 - root_mean_squared_error: 0.8766 - val_loss: 0.8033 - val_root_mean_squared_error: 0.8009\n",
      "Epoch 124/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8785 - root_mean_squared_error: 0.8770 - val_loss: 0.7913 - val_root_mean_squared_error: 0.7841\n",
      "Epoch 125/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8742 - root_mean_squared_error: 0.8737 - val_loss: 0.7935 - val_root_mean_squared_error: 0.7826\n",
      "Epoch 126/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8789 - root_mean_squared_error: 0.8789 - val_loss: 0.7990 - val_root_mean_squared_error: 0.7871\n",
      "Epoch 127/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8710 - root_mean_squared_error: 0.8699 - val_loss: 0.7966 - val_root_mean_squared_error: 0.7843\n",
      "Epoch 128/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8772 - root_mean_squared_error: 0.8761 - val_loss: 0.7952 - val_root_mean_squared_error: 0.7850\n",
      "Epoch 129/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8787 - root_mean_squared_error: 0.8776 - val_loss: 0.7923 - val_root_mean_squared_error: 0.7832\n",
      "Epoch 130/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8781 - root_mean_squared_error: 0.8780 - val_loss: 0.7917 - val_root_mean_squared_error: 0.7802\n",
      "Epoch 131/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8764 - root_mean_squared_error: 0.8736 - val_loss: 0.7957 - val_root_mean_squared_error: 0.7837\n",
      "Epoch 132/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8756 - root_mean_squared_error: 0.8732 - val_loss: 0.7962 - val_root_mean_squared_error: 0.7853\n",
      "Epoch 133/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8762 - root_mean_squared_error: 0.8778 - val_loss: 0.7905 - val_root_mean_squared_error: 0.7809\n",
      "Epoch 134/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8705 - root_mean_squared_error: 0.8705 - val_loss: 0.7895 - val_root_mean_squared_error: 0.7815\n",
      "Epoch 135/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8746 - root_mean_squared_error: 0.8727 - val_loss: 0.7907 - val_root_mean_squared_error: 0.7794\n",
      "Epoch 136/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8771 - root_mean_squared_error: 0.8774 - val_loss: 0.8055 - val_root_mean_squared_error: 0.8028\n",
      "Epoch 137/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8720 - root_mean_squared_error: 0.8714 - val_loss: 0.8006 - val_root_mean_squared_error: 0.7880\n",
      "Epoch 138/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8731 - root_mean_squared_error: 0.8753 - val_loss: 0.8106 - val_root_mean_squared_error: 0.8086\n",
      "Epoch 139/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8717 - root_mean_squared_error: 0.8710 - val_loss: 0.8026 - val_root_mean_squared_error: 0.7981\n",
      "Epoch 140/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8697 - root_mean_squared_error: 0.8675 - val_loss: 0.7926 - val_root_mean_squared_error: 0.7869\n",
      "Epoch 141/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8732 - root_mean_squared_error: 0.8722 - val_loss: 0.8006 - val_root_mean_squared_error: 0.7959\n",
      "Epoch 142/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8745 - root_mean_squared_error: 0.8756 - val_loss: 0.7974 - val_root_mean_squared_error: 0.7937\n",
      "Epoch 143/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8727 - root_mean_squared_error: 0.8717 - val_loss: 0.8068 - val_root_mean_squared_error: 0.8055\n",
      "Epoch 144/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8664 - root_mean_squared_error: 0.8647 - val_loss: 0.7916 - val_root_mean_squared_error: 0.7837\n",
      "Epoch 145/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8721 - root_mean_squared_error: 0.8688 - val_loss: 0.8005 - val_root_mean_squared_error: 0.7859\n",
      "Epoch 146/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8698 - root_mean_squared_error: 0.8683 - val_loss: 0.7980 - val_root_mean_squared_error: 0.7848\n",
      "Epoch 147/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8719 - root_mean_squared_error: 0.8743 - val_loss: 0.7978 - val_root_mean_squared_error: 0.7923\n",
      "Epoch 148/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8679 - root_mean_squared_error: 0.8678 - val_loss: 0.7879 - val_root_mean_squared_error: 0.7780\n",
      "Epoch 149/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8699 - root_mean_squared_error: 0.8700 - val_loss: 0.7924 - val_root_mean_squared_error: 0.7834\n",
      "Epoch 150/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8676 - root_mean_squared_error: 0.8685 - val_loss: 0.7978 - val_root_mean_squared_error: 0.7920\n",
      "Epoch 151/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8720 - root_mean_squared_error: 0.8699 - val_loss: 0.7950 - val_root_mean_squared_error: 0.7825\n",
      "Epoch 152/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8702 - root_mean_squared_error: 0.8707 - val_loss: 0.7962 - val_root_mean_squared_error: 0.7833\n",
      "Epoch 153/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8713 - root_mean_squared_error: 0.8726 - val_loss: 0.7950 - val_root_mean_squared_error: 0.7909\n",
      "Epoch 154/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8659 - root_mean_squared_error: 0.8646 - val_loss: 0.7908 - val_root_mean_squared_error: 0.7822\n",
      "Epoch 155/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8666 - root_mean_squared_error: 0.8667 - val_loss: 0.8166 - val_root_mean_squared_error: 0.8162\n",
      "Epoch 156/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8678 - root_mean_squared_error: 0.8689 - val_loss: 0.8022 - val_root_mean_squared_error: 0.7998\n",
      "Epoch 157/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8663 - root_mean_squared_error: 0.8640 - val_loss: 0.7968 - val_root_mean_squared_error: 0.7842\n",
      "Epoch 158/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8645 - root_mean_squared_error: 0.8666 - val_loss: 0.7943 - val_root_mean_squared_error: 0.7846\n",
      "Epoch 159/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8654 - root_mean_squared_error: 0.8669 - val_loss: 0.8072 - val_root_mean_squared_error: 0.8052\n",
      "Epoch 160/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8638 - root_mean_squared_error: 0.8630 - val_loss: 0.7966 - val_root_mean_squared_error: 0.7901\n",
      "Epoch 161/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8669 - root_mean_squared_error: 0.8673 - val_loss: 0.8094 - val_root_mean_squared_error: 0.8072\n",
      "Epoch 162/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8613 - root_mean_squared_error: 0.8608 - val_loss: 0.7932 - val_root_mean_squared_error: 0.7867\n",
      "Epoch 163/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8652 - root_mean_squared_error: 0.8633 - val_loss: 0.7943 - val_root_mean_squared_error: 0.7815\n",
      "Epoch 164/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8639 - root_mean_squared_error: 0.8647 - val_loss: 0.7898 - val_root_mean_squared_error: 0.7825\n",
      "Epoch 165/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8640 - root_mean_squared_error: 0.8619 - val_loss: 0.7921 - val_root_mean_squared_error: 0.7867\n",
      "Epoch 166/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8641 - root_mean_squared_error: 0.8629 - val_loss: 0.7893 - val_root_mean_squared_error: 0.7773\n",
      "Epoch 167/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8627 - root_mean_squared_error: 0.8638 - val_loss: 0.7872 - val_root_mean_squared_error: 0.7772\n",
      "Epoch 168/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8627 - root_mean_squared_error: 0.8623 - val_loss: 0.7876 - val_root_mean_squared_error: 0.7772\n",
      "Epoch 169/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8596 - root_mean_squared_error: 0.8605 - val_loss: 0.7873 - val_root_mean_squared_error: 0.7792\n",
      "Epoch 170/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8615 - root_mean_squared_error: 0.8592 - val_loss: 0.7881 - val_root_mean_squared_error: 0.7778\n",
      "Epoch 171/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8638 - root_mean_squared_error: 0.8641 - val_loss: 0.8047 - val_root_mean_squared_error: 0.8027\n",
      "Epoch 172/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8634 - root_mean_squared_error: 0.8650 - val_loss: 0.7886 - val_root_mean_squared_error: 0.7812\n",
      "Epoch 173/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8598 - root_mean_squared_error: 0.8585 - val_loss: 0.8243 - val_root_mean_squared_error: 0.8250\n",
      "Epoch 174/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8592 - root_mean_squared_error: 0.8621 - val_loss: 0.7922 - val_root_mean_squared_error: 0.7870\n",
      "Epoch 175/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8595 - root_mean_squared_error: 0.8606 - val_loss: 0.7903 - val_root_mean_squared_error: 0.7837\n",
      "Epoch 176/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8607 - root_mean_squared_error: 0.8628 - val_loss: 0.7872 - val_root_mean_squared_error: 0.7761\n",
      "Epoch 177/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8612 - root_mean_squared_error: 0.8605 - val_loss: 0.7895 - val_root_mean_squared_error: 0.7828\n",
      "Epoch 178/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8584 - root_mean_squared_error: 0.8576 - val_loss: 0.7913 - val_root_mean_squared_error: 0.7856\n",
      "Epoch 179/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8581 - root_mean_squared_error: 0.8559 - val_loss: 0.7954 - val_root_mean_squared_error: 0.7809\n",
      "Epoch 180/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8639 - root_mean_squared_error: 0.8633 - val_loss: 0.7844 - val_root_mean_squared_error: 0.7751\n",
      "Epoch 181/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8628 - root_mean_squared_error: 0.8636 - val_loss: 0.8227 - val_root_mean_squared_error: 0.8203\n",
      "Epoch 182/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8601 - root_mean_squared_error: 0.8616 - val_loss: 0.7892 - val_root_mean_squared_error: 0.7778\n",
      "Epoch 183/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8605 - root_mean_squared_error: 0.8618 - val_loss: 0.8070 - val_root_mean_squared_error: 0.8027\n",
      "Epoch 184/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8572 - root_mean_squared_error: 0.8559 - val_loss: 0.7864 - val_root_mean_squared_error: 0.7785\n",
      "Epoch 185/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8587 - root_mean_squared_error: 0.8549 - val_loss: 0.7897 - val_root_mean_squared_error: 0.7808\n",
      "Epoch 186/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8584 - root_mean_squared_error: 0.8582 - val_loss: 0.7898 - val_root_mean_squared_error: 0.7829\n",
      "Epoch 187/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8558 - root_mean_squared_error: 0.8567 - val_loss: 0.7887 - val_root_mean_squared_error: 0.7794\n",
      "Epoch 188/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8572 - root_mean_squared_error: 0.8591 - val_loss: 0.8100 - val_root_mean_squared_error: 0.8081\n",
      "Epoch 189/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8564 - root_mean_squared_error: 0.8577 - val_loss: 0.7888 - val_root_mean_squared_error: 0.7809\n",
      "Epoch 190/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8545 - root_mean_squared_error: 0.8537 - val_loss: 0.7892 - val_root_mean_squared_error: 0.7810\n",
      "Epoch 191/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8592 - root_mean_squared_error: 0.8565 - val_loss: 0.7917 - val_root_mean_squared_error: 0.7811\n",
      "Epoch 192/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8595 - root_mean_squared_error: 0.8585 - val_loss: 0.7995 - val_root_mean_squared_error: 0.7852\n",
      "Epoch 193/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8551 - root_mean_squared_error: 0.8558 - val_loss: 0.7859 - val_root_mean_squared_error: 0.7761\n",
      "Epoch 194/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8536 - root_mean_squared_error: 0.8542 - val_loss: 0.8218 - val_root_mean_squared_error: 0.8209\n",
      "Epoch 195/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8572 - root_mean_squared_error: 0.8586 - val_loss: 0.7883 - val_root_mean_squared_error: 0.7792\n",
      "Epoch 196/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8601 - root_mean_squared_error: 0.8610 - val_loss: 0.7934 - val_root_mean_squared_error: 0.7878\n",
      "Epoch 197/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8543 - root_mean_squared_error: 0.8551 - val_loss: 0.7894 - val_root_mean_squared_error: 0.7812\n",
      "Epoch 198/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8563 - root_mean_squared_error: 0.8546 - val_loss: 0.7857 - val_root_mean_squared_error: 0.7762\n",
      "Epoch 199/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8527 - root_mean_squared_error: 0.8546 - val_loss: 0.8265 - val_root_mean_squared_error: 0.8255\n",
      "Epoch 200/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8564 - root_mean_squared_error: 0.8561 - val_loss: 0.7921 - val_root_mean_squared_error: 0.7848\n",
      "Epoch 201/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8547 - root_mean_squared_error: 0.8563 - val_loss: 0.8122 - val_root_mean_squared_error: 0.8098\n",
      "Epoch 202/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8551 - root_mean_squared_error: 0.8563 - val_loss: 0.7987 - val_root_mean_squared_error: 0.7926\n",
      "Epoch 203/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8562 - root_mean_squared_error: 0.8571 - val_loss: 0.7931 - val_root_mean_squared_error: 0.7875\n",
      "Epoch 204/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8556 - root_mean_squared_error: 0.8554 - val_loss: 0.8017 - val_root_mean_squared_error: 0.7973\n",
      "Epoch 205/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8479 - root_mean_squared_error: 0.8467 - val_loss: 0.8542 - val_root_mean_squared_error: 0.8570\n",
      "Epoch 206/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8526 - root_mean_squared_error: 0.8544 - val_loss: 0.7936 - val_root_mean_squared_error: 0.7896\n",
      "Epoch 207/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8539 - root_mean_squared_error: 0.8534 - val_loss: 0.8361 - val_root_mean_squared_error: 0.8394\n",
      "Epoch 208/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8543 - root_mean_squared_error: 0.8538 - val_loss: 0.7869 - val_root_mean_squared_error: 0.7803\n",
      "Epoch 209/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8548 - root_mean_squared_error: 0.8548 - val_loss: 0.7990 - val_root_mean_squared_error: 0.7943\n",
      "Epoch 210/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8509 - root_mean_squared_error: 0.8513 - val_loss: 0.7907 - val_root_mean_squared_error: 0.7850\n",
      "Epoch 211/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8559 - root_mean_squared_error: 0.8540 - val_loss: 0.7873 - val_root_mean_squared_error: 0.7729\n",
      "Epoch 212/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8509 - root_mean_squared_error: 0.8505 - val_loss: 0.8017 - val_root_mean_squared_error: 0.7990\n",
      "Epoch 213/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8534 - root_mean_squared_error: 0.8523 - val_loss: 0.8208 - val_root_mean_squared_error: 0.8212\n",
      "Epoch 214/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8491 - root_mean_squared_error: 0.8470 - val_loss: 0.7869 - val_root_mean_squared_error: 0.7752\n",
      "Epoch 215/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8484 - root_mean_squared_error: 0.8499 - val_loss: 0.7962 - val_root_mean_squared_error: 0.7913\n",
      "Epoch 216/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8523 - root_mean_squared_error: 0.8521 - val_loss: 0.7844 - val_root_mean_squared_error: 0.7734\n",
      "Epoch 217/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8524 - root_mean_squared_error: 0.8522 - val_loss: 0.7869 - val_root_mean_squared_error: 0.7787\n",
      "Epoch 218/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8531 - root_mean_squared_error: 0.8546 - val_loss: 0.7953 - val_root_mean_squared_error: 0.7920\n",
      "Epoch 219/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8508 - root_mean_squared_error: 0.8522 - val_loss: 0.7931 - val_root_mean_squared_error: 0.7864\n",
      "Epoch 220/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8476 - root_mean_squared_error: 0.8479 - val_loss: 0.8226 - val_root_mean_squared_error: 0.8216\n",
      "Epoch 221/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8474 - root_mean_squared_error: 0.8474 - val_loss: 0.7917 - val_root_mean_squared_error: 0.7794\n",
      "Epoch 222/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8497 - root_mean_squared_error: 0.8485 - val_loss: 0.7942 - val_root_mean_squared_error: 0.7896\n",
      "Epoch 223/1000\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.8470 - root_mean_squared_error: 0.8467 - val_loss: 0.7904 - val_root_mean_squared_error: 0.7821\n",
      "Epoch 224/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8481 - root_mean_squared_error: 0.8473 - val_loss: 0.8055 - val_root_mean_squared_error: 0.8036\n",
      "Epoch 225/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8517 - root_mean_squared_error: 0.8508 - val_loss: 0.8033 - val_root_mean_squared_error: 0.7987\n",
      "Epoch 226/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8486 - root_mean_squared_error: 0.8475 - val_loss: 0.7857 - val_root_mean_squared_error: 0.7746\n",
      "Epoch 227/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8502 - root_mean_squared_error: 0.8493 - val_loss: 0.7873 - val_root_mean_squared_error: 0.7773\n",
      "Epoch 228/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8495 - root_mean_squared_error: 0.8488 - val_loss: 0.7970 - val_root_mean_squared_error: 0.7918\n",
      "Epoch 229/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8456 - root_mean_squared_error: 0.8455 - val_loss: 0.7865 - val_root_mean_squared_error: 0.7772\n",
      "Epoch 230/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8499 - root_mean_squared_error: 0.8514 - val_loss: 0.8168 - val_root_mean_squared_error: 0.8150\n",
      "Epoch 231/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8487 - root_mean_squared_error: 0.8473 - val_loss: 0.7903 - val_root_mean_squared_error: 0.7765\n",
      "Epoch 232/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8491 - root_mean_squared_error: 0.8501 - val_loss: 0.7832 - val_root_mean_squared_error: 0.7734\n",
      "Epoch 233/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8458 - root_mean_squared_error: 0.8463 - val_loss: 0.7906 - val_root_mean_squared_error: 0.7767\n",
      "Epoch 234/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8445 - root_mean_squared_error: 0.8412 - val_loss: 0.7950 - val_root_mean_squared_error: 0.7804\n",
      "Epoch 235/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8485 - root_mean_squared_error: 0.8483 - val_loss: 0.7868 - val_root_mean_squared_error: 0.7780\n",
      "Epoch 236/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8448 - root_mean_squared_error: 0.8423 - val_loss: 0.7828 - val_root_mean_squared_error: 0.7761\n",
      "Epoch 237/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8449 - root_mean_squared_error: 0.8444 - val_loss: 0.7937 - val_root_mean_squared_error: 0.7888\n",
      "Epoch 238/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8466 - root_mean_squared_error: 0.8463 - val_loss: 0.7887 - val_root_mean_squared_error: 0.7791\n",
      "Epoch 239/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8458 - root_mean_squared_error: 0.8450 - val_loss: 0.8030 - val_root_mean_squared_error: 0.8001\n",
      "Epoch 240/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8452 - root_mean_squared_error: 0.8461 - val_loss: 0.7871 - val_root_mean_squared_error: 0.7808\n",
      "Epoch 241/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8460 - root_mean_squared_error: 0.8444 - val_loss: 0.7934 - val_root_mean_squared_error: 0.7858\n",
      "Epoch 242/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8474 - root_mean_squared_error: 0.8460 - val_loss: 0.7924 - val_root_mean_squared_error: 0.7778\n",
      "Epoch 243/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8453 - root_mean_squared_error: 0.8466 - val_loss: 0.7883 - val_root_mean_squared_error: 0.7812\n",
      "Epoch 244/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8440 - root_mean_squared_error: 0.8447 - val_loss: 0.7952 - val_root_mean_squared_error: 0.7914\n",
      "Epoch 245/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8454 - root_mean_squared_error: 0.8453 - val_loss: 0.7952 - val_root_mean_squared_error: 0.7821\n",
      "Epoch 246/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8497 - root_mean_squared_error: 0.8500 - val_loss: 0.7922 - val_root_mean_squared_error: 0.7865\n",
      "Epoch 247/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8384 - root_mean_squared_error: 0.8388 - val_loss: 0.8251 - val_root_mean_squared_error: 0.8226\n",
      "Epoch 248/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8442 - root_mean_squared_error: 0.8464 - val_loss: 0.8011 - val_root_mean_squared_error: 0.7963\n",
      "Epoch 249/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8448 - root_mean_squared_error: 0.8468 - val_loss: 0.7890 - val_root_mean_squared_error: 0.7818\n",
      "Epoch 250/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8446 - root_mean_squared_error: 0.8442 - val_loss: 0.7988 - val_root_mean_squared_error: 0.7916\n",
      "Epoch 251/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8447 - root_mean_squared_error: 0.8455 - val_loss: 0.7919 - val_root_mean_squared_error: 0.7848\n",
      "Epoch 252/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8409 - root_mean_squared_error: 0.8396 - val_loss: 0.8062 - val_root_mean_squared_error: 0.8014\n",
      "Epoch 253/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8449 - root_mean_squared_error: 0.8443 - val_loss: 0.8097 - val_root_mean_squared_error: 0.8049\n",
      "Epoch 254/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8429 - root_mean_squared_error: 0.8416 - val_loss: 0.8010 - val_root_mean_squared_error: 0.7835\n",
      "Epoch 255/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8416 - root_mean_squared_error: 0.8425 - val_loss: 0.7843 - val_root_mean_squared_error: 0.7709\n",
      "Epoch 256/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8439 - root_mean_squared_error: 0.8435 - val_loss: 0.8067 - val_root_mean_squared_error: 0.8024\n",
      "Epoch 257/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8442 - root_mean_squared_error: 0.8470 - val_loss: 0.7993 - val_root_mean_squared_error: 0.7938\n",
      "Epoch 258/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8413 - root_mean_squared_error: 0.8394 - val_loss: 0.8061 - val_root_mean_squared_error: 0.8033\n",
      "Epoch 259/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8369 - root_mean_squared_error: 0.8392 - val_loss: 0.8036 - val_root_mean_squared_error: 0.7989\n",
      "Epoch 260/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8418 - root_mean_squared_error: 0.8405 - val_loss: 0.7816 - val_root_mean_squared_error: 0.7687\n",
      "Epoch 261/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8423 - root_mean_squared_error: 0.8398 - val_loss: 0.7857 - val_root_mean_squared_error: 0.7738\n",
      "Epoch 262/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8425 - root_mean_squared_error: 0.8393 - val_loss: 0.7897 - val_root_mean_squared_error: 0.7788\n",
      "Epoch 263/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8397 - root_mean_squared_error: 0.8385 - val_loss: 0.8062 - val_root_mean_squared_error: 0.8034\n",
      "Epoch 264/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8448 - root_mean_squared_error: 0.8456 - val_loss: 0.7871 - val_root_mean_squared_error: 0.7768\n",
      "Epoch 265/1000\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.8489 - root_mean_squared_error: 0.8489"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-da8b799ce7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                             np.asarray(sim_rest)[2000:]] ,\n\u001b[1;32m     37\u001b[0m                             np.asarray(curr['score_avg'])[2000:]),  \n\u001b[0;32m---> 38\u001b[0;31m           epochs=1000, verbose = 1)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    303\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3519\u001b[0m     \"\"\"\n\u001b[1;32m   3520\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3521\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3427\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3430\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3568\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m         \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m     \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(model.summary())\n",
    "# model.compile(optimizer=Adam(lr=0.01), \n",
    "#               loss='categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "# model.fit([np.asarray(arg1)[:2000], np.asarray(arg0)[:2000],np.asarray(rest)[:2000],np.asarray(default)[:2000]] ,s[:2000], validation_data= ([np.asarray(arg1)[2000:], np.asarray(arg0)[2000:], np.asarray(rest)[2000:], np.asarray(default)[2000:]],  s[2000:]),  epochs=100)\n",
    "\n",
    "print(model.summary())\n",
    "# model.compile(optimizer= 'sgd', loss='mse', metrics=['mse'])\n",
    "from keras.optimizers import Adam, SGD\n",
    "opt = SGD(lr=0.01)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "model.compile(optimizer = \"rmsprop\", loss = root_mean_squared_error, \n",
    "              metrics =root_mean_squared_error)\n",
    "\n",
    "history = model.fit([\n",
    "    np.asarray(arg1)[:2000], \n",
    "           np.asarray(arg0)[:2000],\n",
    "           np.asarray(rest)[:2000],\n",
    "           np.asarray(default)[:2000], \n",
    "           np.asarray(sim_default)[:2000],\n",
    "           np.asarray(sim_arg0)[:2000],\n",
    "           np.asarray(sim_arg1)[:2000],\n",
    "           np.asarray(sim_rest)[:2000]] ,\n",
    "          np.asarray(curr['score_avg'])[:2000], \n",
    "          validation_data= ([\n",
    "              np.asarray(arg1)[2000:], \n",
    "                             np.asarray(arg0)[2000:], \n",
    "                             np.asarray(rest)[2000:], \n",
    "                             np.asarray(default)[2000:],\n",
    "                            np.asarray(sim_default)[2000:],\n",
    "                            np.asarray(sim_arg0)[2000:],\n",
    "                            np.asarray(sim_arg1)[2000:],\n",
    "                            np.asarray(sim_rest)[2000:]] ,\n",
    "                            np.asarray(curr['score_avg'])[2000:]),  \n",
    "          epochs=1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "87112bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.5  , 5.   , 4.   , 3.   , 2.   , 2.5  , 4.5  , 1.5  , 1.   ,\n",
       "       0.   , 0.5  , 2.625, 3.75 , 1.375, 2.875, 4.375, 2.75 , 2.25 ,\n",
       "       4.625, 4.875, 0.625, 1.25 , 3.375, 2.375, 1.125, 1.75 , 1.625,\n",
       "       4.75 , 1.875, 4.25 , 3.625, 2.125, 4.125, 3.25 , 3.875, 3.125])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.array(curr['score_avg'])).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "26a44d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr['score_ceil'] =  np.ceil(curr['score_me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "31951bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_categorical(np.array(curr['score_ceil']), num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "159014a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "fab54110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ac5235df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.105878 ],\n",
       "       [4.1982136],\n",
       "       [3.9925683],\n",
       "       ...,\n",
       "       [3.9402134],\n",
       "       [4.015551 ],\n",
       "       [3.9436562]], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([np.asarray(arg1), (np.asarray(arg0)),(np.asarray(arg0)),(np.asarray(arg0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5c72a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = curr.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10bb4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f271504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2268, 30)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff3a7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589     [ 0.63212536  0.62462431  0.60469761  0.572757...\n",
       "1294    [ 0.57402884  0.56823161  0.55281473  0.528052...\n",
       "2220    [ 5.83284770e-01  5.77061762e-01  5.60521540e-...\n",
       "192     [ 5.81695204e-01  5.75183351e-01  5.57888505e-...\n",
       "1032    [ 0.62872611  0.62081352  0.59980995  0.566194...\n",
       "                              ...                        \n",
       "684     [ 5.66622939e-01  5.61018534e-01  5.46112634e-...\n",
       "2143    [ 5.94684091e-01  5.87925708e-01  5.69975372e-...\n",
       "1544    [ 4.99969827e-01  4.96100881e-01  4.85793730e-...\n",
       "391     [ 0.5777289   0.57146577  0.55482415  0.528138...\n",
       "141     [ 0.63228819  0.62399583  0.60200261  0.566859...\n",
       "Name: student_amr_ARG1, Length: 2268, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr['student_amr_ARG1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "573859a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg1=[]\n",
    "# for i, j in curr.iterrows():\n",
    "#     arg1.append(np.exp(-np.abs(j['model_amr_ARG1'] - j['student_amr_ARG1'])))\n",
    "\n",
    "arg1=[]\n",
    "for i, j in curr.iterrows():\n",
    "    arg1.append(np.concatenate([j['model_amr_ARG1'].reshape(500,),j['student_amr_ARG1'].reshape(500,)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0fd3bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg0=[]\n",
    "# for i, j in curr.iterrows():\n",
    "#     arg0.append(np.exp(-np.abs(j['model_amr_ARG0'] - j['student_amr_ARG0'])))\n",
    "    \n",
    "arg0=[]\n",
    "for i, j in curr.iterrows():\n",
    "    arg0.append(np.concatenate([j['model_amr_ARG0'],j['student_amr_ARG0']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "214343f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default=[]\n",
    "# for i, j in curr.iterrows():\n",
    "#     default.append(np.exp(-np.abs(j['model_amr_default'] - j['student_amr_default'])))\n",
    "    \n",
    "default=[]\n",
    "for i, j in curr.iterrows():\n",
    "    default.append(np.concatenate([j['model_amr_default'],j['student_amr_default']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fd49d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest=[]\n",
    "# for i, j in curr.iterrows():\n",
    "#     rest.append(np.exp(-np.abs(j['model_amr_rest'] - j['student_amr_rest'])))\n",
    "    \n",
    "rest=[]\n",
    "for i, j in curr.iterrows():\n",
    "    rest.append(np.concatenate([j['model_amr_rest'], j['student_amr_rest']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be6d5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_default, sim_arg0, sim_arg1, sim_rest=[], [], [], []\n",
    "for i, j in curr.iterrows():\n",
    "    sim_default.append(curr.at[i,'similarity_default'])\n",
    "    sim_rest.append(curr.at[i,'similarity_rest'])\n",
    "    sim_arg0.append(curr.at[i,'similarity_ARG0'])\n",
    "    sim_arg1.append(curr.at[i,'similarity_ARG1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d476feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2268, 1000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(arg1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e3cf4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat  = tf.keras.layers.concatenate([preds, preds_2],axis=1)\n",
    "# dense1_layer = tf.keras.layers.Dense(units=1)\n",
    "# output = dense1_layer(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9caa76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Model(inputs=[input1, input2], outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8b12c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a6aafb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "347669e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"first60_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "df75d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3030926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ac8f23bb33f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_amr_ARG1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'student_amr_ARG1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "np.concatenate(np.asarray(curr['model_amr_ARG1']), np.asarray(curr['student_amr_ARG1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "03f3837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8b6dc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "310b1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr[\"model_amr_ARG0\"][0].reshape(500,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "26353c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "curr= df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "87871c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate((np.asarray(curr[\"model_amr_ARG0\"]).reshape(58, 1),np.asarray(curr[\"model_amr_ARG1\"]).reshape(58,1)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab65b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.concatenate((np.asarray(curr[\"student_amr_ARG0\"]).reshape(58, 1),np.asarray(curr[\"student_amr_ARG1\"]).reshape(58,1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c809ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.concatenate((a,b), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aa866b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in a:\n",
    "    f = []\n",
    "    for e in i:\n",
    "        f.append(e.tolist())\n",
    "    d.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5a5bfb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "bd5e06b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr[\"model_amr_ARG0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4209ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18d016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4da01c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "882707a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [list(arg0_model_embedding), list(arg0_student_embedding), list(arg1_model_embedding), list(arg1_student_embedding)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f01adfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa8ffc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/Users/varishgrover/downloads/mohler_dataset_edited.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5834ad62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2273, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca6ed23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del df['student_amr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b97ba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ::snt High risk problems are address in the prototype program to make sure that the program is feasible.  A prototype may also be used to show a company that the software can be possibly programmed.\\n(m / multi-sentence\\n      :snt1 (a / address-02\\n            :ARG0 (p / program\\n                  :mod (p2 / prototype))\\n            :ARG1 (p3 / problem\\n                  :mod (r / risk-01\\n                        :ARG1-of (h / high-02)))\\n            :purpose (e / ensure-01\\n                  :ARG0 p\\n                  :ARG1 (f / feasible\\n                        :domain (p4 / program))))\\n      :snt2 (p5 / possible-01\\n            :ARG1 (u / use-01\\n                  :ARG1 (p6 / prototype)\\n                  :ARG2 (s / show-01\\n                        :ARG0 p6\\n                        :ARG1 (p7 / possible-01\\n                              :ARG1 (p8 / program-01\\n                                    :ARG1 (s2 / software)))\\n                        :ARG2 (c / company))\\n                  :mod (a2 / also))))'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().iloc[0]['student_amr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92c5b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>desired_answer</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>score_me</th>\n",
       "      <th>score_other</th>\n",
       "      <th>score_avg</th>\n",
       "      <th>student_amr</th>\n",
       "      <th>model_amr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>High risk problems are address in the prototyp...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td># ::snt High risk problems are address in the ...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>To simulate portions of the desired final prod...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td># ::snt To simulate portions of the desired fi...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>A prototype program simulates the behaviors of...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td># ::snt A prototype program simulates the beha...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>Defined in the Specification phase a prototype...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td># ::snt Defined in the Specification phase a p...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>It is used to let the users have a first idea ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td># ::snt It is used to let the users have a fir...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   id                                           question  \\\n",
       "0           0  1.1  What is the role of a prototype program in pro...   \n",
       "1           1  1.1  What is the role of a prototype program in pro...   \n",
       "2           2  1.1  What is the role of a prototype program in pro...   \n",
       "3           3  1.1  What is the role of a prototype program in pro...   \n",
       "4           4  1.1  What is the role of a prototype program in pro...   \n",
       "\n",
       "                                      desired_answer  \\\n",
       "0  To simulate the behaviour of portions of the d...   \n",
       "1  To simulate the behaviour of portions of the d...   \n",
       "2  To simulate the behaviour of portions of the d...   \n",
       "3  To simulate the behaviour of portions of the d...   \n",
       "4  To simulate the behaviour of portions of the d...   \n",
       "\n",
       "                                      student_answer  score_me  score_other  \\\n",
       "0  High risk problems are address in the prototyp...       4.0          3.0   \n",
       "1  To simulate portions of the desired final prod...       5.0          5.0   \n",
       "2  A prototype program simulates the behaviors of...       5.0          3.0   \n",
       "3  Defined in the Specification phase a prototype...       5.0          5.0   \n",
       "4  It is used to let the users have a first idea ...       3.0          3.0   \n",
       "\n",
       "   score_avg                                        student_amr  \\\n",
       "0        3.5  # ::snt High risk problems are address in the ...   \n",
       "1        5.0  # ::snt To simulate portions of the desired fi...   \n",
       "2        4.0  # ::snt A prototype program simulates the beha...   \n",
       "3        5.0  # ::snt Defined in the Specification phase a p...   \n",
       "4        3.0  # ::snt It is used to let the users have a fir...   \n",
       "\n",
       "                                           model_amr  \n",
       "0  # ::snt To simulate the behaviour of portions ...  \n",
       "1  # ::snt To simulate the behaviour of portions ...  \n",
       "2  # ::snt To simulate the behaviour of portions ...  \n",
       "3  # ::snt To simulate the behaviour of portions ...  \n",
       "4  # ::snt To simulate the behaviour of portions ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51410e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varishgrover/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but define-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but dimension is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but left-19 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but remove-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but you is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but link-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but or is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but sort-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n"
     ]
    }
   ],
   "source": [
    "# import amrlib\n",
    "# stog = amrlib.load_stog_model()\n",
    "# df= pd.read_csv(\"/Users/varishgrover/downloads/mohler_dataset_edited.csv\")\n",
    "count = 0\n",
    "s  = set()\n",
    "for i, j in df.iterrows():\n",
    "    if j['desired_answer'] not in s:\n",
    "        s.add(j['desired_answer'])\n",
    "        graphs = stog.parse_sents([j['desired_answer']])\n",
    "        df.at[i, 'model_amr'] = graphs[0]\n",
    "    else:\n",
    "        df.at[i, 'model_amr'] = df.at[i - 1, 'model_amr']\n",
    "    count+=1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ce35fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"amr_mohler.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d38db52f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curr = df[(df.id == 1.1)|((df.id == 1.2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b718791",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = stog.parse_sents([curr.iloc[0]['desired_answer'], curr.iloc[56]['desired_answer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a14ed448",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [graphs[0]]*29 + [graphs[1]]*29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79ceb4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "020699db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[man, program, hacker] [woman, program, hacker]\n",
    "[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e43934dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varishgrover/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "curr['model_amr'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82dfc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/varishgrover/downloads/mohler_dataset_edited_amr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731692e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3b2211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>desired_answer</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>score_me</th>\n",
       "      <th>score_other</th>\n",
       "      <th>score_avg</th>\n",
       "      <th>student_amr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>High risk problems are address in the prototyp...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td># ::snt High risk problems are address in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>To simulate portions of the desired final prod...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td># ::snt To simulate portions of the desired fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>A prototype program simulates the behaviors of...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td># ::snt A prototype program simulates the beha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>Defined in the Specification phase a prototype...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td># ::snt Defined in the Specification phase a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>It is used to let the users have a first idea ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td># ::snt It is used to let the users have a fir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           question  \\\n",
       "0  1.1  What is the role of a prototype program in pro...   \n",
       "1  1.1  What is the role of a prototype program in pro...   \n",
       "2  1.1  What is the role of a prototype program in pro...   \n",
       "3  1.1  What is the role of a prototype program in pro...   \n",
       "4  1.1  What is the role of a prototype program in pro...   \n",
       "\n",
       "                                      desired_answer  \\\n",
       "0  To simulate the behaviour of portions of the d...   \n",
       "1  To simulate the behaviour of portions of the d...   \n",
       "2  To simulate the behaviour of portions of the d...   \n",
       "3  To simulate the behaviour of portions of the d...   \n",
       "4  To simulate the behaviour of portions of the d...   \n",
       "\n",
       "                                      student_answer  score_me  score_other  \\\n",
       "0  High risk problems are address in the prototyp...       4.0          3.0   \n",
       "1  To simulate portions of the desired final prod...       5.0          5.0   \n",
       "2  A prototype program simulates the behaviors of...       5.0          3.0   \n",
       "3  Defined in the Specification phase a prototype...       5.0          5.0   \n",
       "4  It is used to let the users have a first idea ...       3.0          3.0   \n",
       "\n",
       "   score_avg                                        student_amr  \n",
       "0        3.5  # ::snt High risk problems are address in the ...  \n",
       "1        5.0  # ::snt To simulate portions of the desired fi...  \n",
       "2        4.0  # ::snt A prototype program simulates the beha...  \n",
       "3        5.0  # ::snt Defined in the Specification phase a p...  \n",
       "4        3.0  # ::snt It is used to let the users have a fir...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113a240",
   "metadata": {},
   "source": [
    "# Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea8870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "curr = pd.read_csv(\"graph_and_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f9e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>desired_answer</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>score_me</th>\n",
       "      <th>score_other</th>\n",
       "      <th>score_avg</th>\n",
       "      <th>student_amr</th>\n",
       "      <th>model_amr</th>\n",
       "      <th>model_amr_ARG1</th>\n",
       "      <th>...</th>\n",
       "      <th>model_amr_rest_text</th>\n",
       "      <th>student_amr_rest_text</th>\n",
       "      <th>model_amr_ARG0_text</th>\n",
       "      <th>student_amr_ARG0_text</th>\n",
       "      <th>model_amr_ARG1_text</th>\n",
       "      <th>student_amr_ARG1_text</th>\n",
       "      <th>similarity_default</th>\n",
       "      <th>similarity_ARG1</th>\n",
       "      <th>similarity_ARG0</th>\n",
       "      <th>similarity_rest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.3</td>\n",
       "      <td>How are objects initialized when they are crea...</td>\n",
       "      <td>By using constructors.</td>\n",
       "      <td>The value is specified after declaration,  It ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.375</td>\n",
       "      <td># ::snt The value is specified after declarati...</td>\n",
       "      <td># ::snt By using constructors.\\n(u / use-01\\n ...</td>\n",
       "      <td>[ 0.66660261  0.65839501  0.63659099  0.601640...</td>\n",
       "      <td>...</td>\n",
       "      <td>constructor</td>\n",
       "      <td>initialize specify string correspond possible ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constructor</td>\n",
       "      <td>initialize specify correspond possible between...</td>\n",
       "      <td>0.326516</td>\n",
       "      <td>0.353135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.293427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.4</td>\n",
       "      <td>Briefly, how does selection sort work?</td>\n",
       "      <td>It selects the minimum from an array and place...</td>\n",
       "      <td>Selection sort searches the array for the lowe...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.000</td>\n",
       "      <td># ::snt Selection sort searches the array for ...</td>\n",
       "      <td># ::snt It selects the minimum from an array a...</td>\n",
       "      <td>[ 5.55512020e-01  5.49932946e-01  5.35101169e-...</td>\n",
       "      <td>...</td>\n",
       "      <td>ordinal place select et then position array re...</td>\n",
       "      <td>ordinal search have low swap et then array nex...</td>\n",
       "      <td>select place it</td>\n",
       "      <td>search swap sort</td>\n",
       "      <td>place select minimum</td>\n",
       "      <td>search low have select swap sort array value</td>\n",
       "      <td>0.622574</td>\n",
       "      <td>0.425756</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>0.615988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.4</td>\n",
       "      <td>How can you implement a stack with a list?</td>\n",
       "      <td>Keep the top of the stack pointing to the head...</td>\n",
       "      <td>adding the element to the end of the list so t...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.500</td>\n",
       "      <td># ::snt adding the element to the end of the l...</td>\n",
       "      <td># ::snt Keep the top of the stack pointing to ...</td>\n",
       "      <td>[ 5.93086869e-01  5.86630380e-01  5.69470887e-...</td>\n",
       "      <td>...</td>\n",
       "      <td>begin add point push remove or stack operation...</td>\n",
       "      <td>ordinal end take set add pop link element prev...</td>\n",
       "      <td>add point remove operation top you</td>\n",
       "      <td>set take it</td>\n",
       "      <td>begin add point remove link element list</td>\n",
       "      <td>end take set add pop link element list</td>\n",
       "      <td>0.552367</td>\n",
       "      <td>0.647215</td>\n",
       "      <td>0.292711</td>\n",
       "      <td>0.467043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.5</td>\n",
       "      <td>What is a recursive function?</td>\n",
       "      <td>A function that calls itself.</td>\n",
       "      <td>a function that calls itself, and which eaach ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.000</td>\n",
       "      <td># ::snt a function that calls itself, and whic...</td>\n",
       "      <td># ::snt A function that calls itself.\\n(f / fu...</td>\n",
       "      <td>[ 0.66660261  0.65839501  0.63659099  0.601640...</td>\n",
       "      <td>...</td>\n",
       "      <td>call</td>\n",
       "      <td>get case close and base</td>\n",
       "      <td>call</td>\n",
       "      <td>get call each</td>\n",
       "      <td>call</td>\n",
       "      <td>get close call and</td>\n",
       "      <td>0.635330</td>\n",
       "      <td>0.537560</td>\n",
       "      <td>0.712269</td>\n",
       "      <td>0.246872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.1</td>\n",
       "      <td>What is a linked list?</td>\n",
       "      <td>A collection of elements that can be allocated...</td>\n",
       "      <td>A linked list is a chain of structs or records...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.500</td>\n",
       "      <td># ::snt A linked list is a chain of structs or...</td>\n",
       "      <td># ::snt A collection of elements that can be a...</td>\n",
       "      <td>[ 6.66595041e-01  6.57421360e-01  6.33083801e-...</td>\n",
       "      <td>...</td>\n",
       "      <td>allocate possible dynamic</td>\n",
       "      <td>add chain delete retrieve call possible or rec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>delete retrieve add you</td>\n",
       "      <td>allocate possible element</td>\n",
       "      <td>chain delete retrieve add call possible link o...</td>\n",
       "      <td>0.424939</td>\n",
       "      <td>0.307559</td>\n",
       "      <td>0.060696</td>\n",
       "      <td>0.313224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  11.3  How are objects initialized when they are crea...   \n",
       "1  12.4             Briefly, how does selection sort work?   \n",
       "2   8.4         How can you implement a stack with a list?   \n",
       "3  11.5                      What is a recursive function?   \n",
       "4   7.1                             What is a linked list?   \n",
       "\n",
       "                                      desired_answer  \\\n",
       "0                             By using constructors.   \n",
       "1  It selects the minimum from an array and place...   \n",
       "2  Keep the top of the stack pointing to the head...   \n",
       "3                      A function that calls itself.   \n",
       "4  A collection of elements that can be allocated...   \n",
       "\n",
       "                                      student_answer  score_me  score_other  \\\n",
       "0  The value is specified after declaration,  It ...       1.0         3.75   \n",
       "1  Selection sort searches the array for the lowe...       5.0         5.00   \n",
       "2  adding the element to the end of the list so t...       3.0         4.00   \n",
       "3  a function that calls itself, and which eaach ...       5.0         5.00   \n",
       "4  A linked list is a chain of structs or records...       4.0         5.00   \n",
       "\n",
       "   score_avg                                        student_amr  \\\n",
       "0      2.375  # ::snt The value is specified after declarati...   \n",
       "1      5.000  # ::snt Selection sort searches the array for ...   \n",
       "2      3.500  # ::snt adding the element to the end of the l...   \n",
       "3      5.000  # ::snt a function that calls itself, and whic...   \n",
       "4      4.500  # ::snt A linked list is a chain of structs or...   \n",
       "\n",
       "                                           model_amr  \\\n",
       "0  # ::snt By using constructors.\\n(u / use-01\\n ...   \n",
       "1  # ::snt It selects the minimum from an array a...   \n",
       "2  # ::snt Keep the top of the stack pointing to ...   \n",
       "3  # ::snt A function that calls itself.\\n(f / fu...   \n",
       "4  # ::snt A collection of elements that can be a...   \n",
       "\n",
       "                                      model_amr_ARG1  ...  \\\n",
       "0  [ 0.66660261  0.65839501  0.63659099  0.601640...  ...   \n",
       "1  [ 5.55512020e-01  5.49932946e-01  5.35101169e-...  ...   \n",
       "2  [ 5.93086869e-01  5.86630380e-01  5.69470887e-...  ...   \n",
       "3  [ 0.66660261  0.65839501  0.63659099  0.601640...  ...   \n",
       "4  [ 6.66595041e-01  6.57421360e-01  6.33083801e-...  ...   \n",
       "\n",
       "                                 model_amr_rest_text  \\\n",
       "0                                        constructor   \n",
       "1  ordinal place select et then position array re...   \n",
       "2  begin add point push remove or stack operation...   \n",
       "3                                               call   \n",
       "4                          allocate possible dynamic   \n",
       "\n",
       "                               student_amr_rest_text  \\\n",
       "0  initialize specify string correspond possible ...   \n",
       "1  ordinal search have low swap et then array nex...   \n",
       "2  ordinal end take set add pop link element prev...   \n",
       "3                            get case close and base   \n",
       "4  add chain delete retrieve call possible or rec...   \n",
       "\n",
       "                  model_amr_ARG0_text    student_amr_ARG0_text  \\\n",
       "0                                 NaN                      NaN   \n",
       "1                     select place it         search swap sort   \n",
       "2  add point remove operation top you              set take it   \n",
       "3                                call            get call each   \n",
       "4                                 NaN  delete retrieve add you   \n",
       "\n",
       "                        model_amr_ARG1_text  \\\n",
       "0                               constructor   \n",
       "1                      place select minimum   \n",
       "2  begin add point remove link element list   \n",
       "3                                      call   \n",
       "4                 allocate possible element   \n",
       "\n",
       "                               student_amr_ARG1_text similarity_default  \\\n",
       "0  initialize specify correspond possible between...           0.326516   \n",
       "1       search low have select swap sort array value           0.622574   \n",
       "2             end take set add pop link element list           0.552367   \n",
       "3                                 get close call and           0.635330   \n",
       "4  chain delete retrieve add call possible link o...           0.424939   \n",
       "\n",
       "   similarity_ARG1 similarity_ARG0 similarity_rest  \n",
       "0         0.353135        1.000000        0.293427  \n",
       "1         0.425756        0.363637        0.615988  \n",
       "2         0.647215        0.292711        0.467043  \n",
       "3         0.537560        0.712269        0.246872  \n",
       "4         0.307559        0.060696        0.313224  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8546287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "curr = curr.set_index(np.arange(len(curr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44ca7150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curr['model_amr_ARG1'  ] = None\n",
    "curr['student_amr_ARG1'] =None\n",
    "curr['model_amr_ARG0'  ] =None\n",
    "curr['student_amr_ARG0'] =None\n",
    "curr['model_amr_rest']      = None\n",
    "curr['student_amr_rest']    = None\n",
    "curr['model_amr_default']   = None\n",
    "curr['student_amr_default'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "197715e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr['model_amr_default_text']   = None\n",
    "curr['student_amr_default_text'] = None\n",
    "curr['model_amr_rest_text']      = None\n",
    "curr['student_amr_rest_text']    = None\n",
    "curr['model_amr_ARG0_text']      = None\n",
    "curr['student_amr_ARG0_text']    = None\n",
    "curr['model_amr_ARG1_text']      = None\n",
    "curr['student_amr_ARG1_text']    = None\n",
    "curr['model_amr_default_text']   =curr['model_amr_default_text']  .astype(object)\n",
    "curr['student_amr_default_text'] =curr['student_amr_default_text'].astype(object)\n",
    "curr['model_amr_rest_text']      =curr['model_amr_rest_text']     .astype(object)\n",
    "curr['student_amr_rest_text']    =curr['student_amr_rest_text']   .astype(object)\n",
    "curr['model_amr_ARG0_text']      =curr['model_amr_ARG0_text']     .astype(object)\n",
    "curr['student_amr_ARG0_text']    =curr['student_amr_ARG0_text']   .astype(object)\n",
    "curr['model_amr_ARG1_text']      =curr['model_amr_ARG1_text']     .astype(object)\n",
    "curr['student_amr_ARG1_text']    =curr['student_amr_ARG1_text']   .astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2627688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = [],[],[],[]\n",
    "for i, j in curr.iterrows():\n",
    "    curr.at[i, 'model_amr_default_text']    = curr.at[i, 'desired_answer']\n",
    "    curr.at[i, 'student_amr_default_text']  = curr.at[i, 'student_answer']\n",
    "    curr.at[i, 'model_amr_rest_text']       = generate_adjlist_text_rest([j['model_amr']])\n",
    "    curr.at[i, 'student_amr_rest_text']     = generate_adjlist_text_rest([j['student_amr']])\n",
    "    curr.at[i, 'model_amr_ARG0_text']       = generate_adjlist_text('ARG0',[j['model_amr']])\n",
    "    curr.at[i, 'student_amr_ARG0_text']     = generate_adjlist_text('ARG0',[j['student_amr']])\n",
    "    curr.at[i, 'model_amr_ARG1_text']       = generate_adjlist_text('ARG1',[j['model_amr']  ])\n",
    "    curr.at[i, 'student_amr_ARG1_text']     = generate_adjlist_text('ARG1', [j['student_amr']])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "eef6c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr.to_csv(\"graph_and_text.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57416a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr['model_amr_ARG1'  ] = curr['model_amr_ARG1'  ].astype(object) \n",
    "curr['student_amr_ARG1'] = curr['student_amr_ARG1'].astype(object)\n",
    "curr['model_amr_ARG0'  ] = curr['model_amr_ARG0'  ].astype(object) \n",
    "curr['student_amr_ARG0'] = curr['student_amr_ARG0'].astype(object) \n",
    "curr['model_amr_rest']       = curr['model_amr_rest'].astype(object) \n",
    "curr['student_amr_rest']     = curr['student_amr_rest'].astype(object)\n",
    "curr['model_amr_default']    = curr['model_amr_default'].astype(object) \n",
    "curr['student_amr_default']  = curr['student_amr_default'].astype(object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ea4e0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = [],[],[],[]\n",
    "for i, j in curr.iterrows():\n",
    "    curr.at[i,'model_amr_ARG1' ]  = (generate_embedding(generate_adjlist('ARG1', [j['model_amr']])).reshape(500,))\n",
    "    curr.at[i,'student_amr_ARG1'] = (generate_embedding(generate_adjlist('ARG1', [j['student_amr']])).reshape(500,))\n",
    "    curr.at[i,'model_amr_ARG0' ]  = (generate_embedding(generate_adjlist('ARG0', [j['model_amr']])).reshape(500,))\n",
    "    curr.at[i,'student_amr_ARG0'] = (generate_embedding(generate_adjlist('ARG0', [j['student_amr']])).reshape(500,))\n",
    "    curr.at[i,'model_amr_rest']        = (generate_embedding(generate_adjlist_compliment([j['model_amr']])).reshape(500,))\n",
    "    curr.at[i,'student_amr_rest']      = (generate_embedding(generate_adjlist_compliment([j['student_amr']])).reshape(500,))\n",
    "    curr.at[i,'model_amr_default']     = (generate_embedding(generate_adjlist_default([j['model_amr']  ])).reshape(500,))\n",
    "    curr.at[i,'student_amr_default']   = (generate_embedding(generate_adjlist_default([j['student_amr']])).reshape(500,))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "2b7d00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr.to_csv(\"graph_and_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921bac6",
   "metadata": {},
   "source": [
    "# Text Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "5cbf3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "94badc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulate portions behaviour desired software product \t\t behaviour simulate portions product software desired \t\t Score: 0.9664\n"
     ]
    }
   ],
   "source": [
    "# Two lists of sentences\n",
    "sentences1 = ['simulate portions behaviour desired software product']\n",
    "sentences2 = ['behaviour simulate portions product software desired']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3a81df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = list(curr['model_amr_rest_text'])\n",
    "sentences2 = list(curr['student_amr_rest_text'])\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "7aa7db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(len(sentences1)):\n",
    "    scores.append(cosine_scores[i][i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "b221bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr['similarity_rest'] = np.asarray(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "87467007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>desired_answer</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>score_me</th>\n",
       "      <th>score_other</th>\n",
       "      <th>score_avg</th>\n",
       "      <th>student_amr</th>\n",
       "      <th>model_amr</th>\n",
       "      <th>model_amr_ARG1</th>\n",
       "      <th>...</th>\n",
       "      <th>model_amr_rest_text</th>\n",
       "      <th>student_amr_rest_text</th>\n",
       "      <th>model_amr_ARG0_text</th>\n",
       "      <th>student_amr_ARG0_text</th>\n",
       "      <th>model_amr_ARG1_text</th>\n",
       "      <th>student_amr_ARG1_text</th>\n",
       "      <th>similarity_default</th>\n",
       "      <th>similarity_ARG1</th>\n",
       "      <th>similarity_ARG0</th>\n",
       "      <th>similarity_rest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>High risk problems are address in the prototyp...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td># ::snt High risk problems are address in the ...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "      <td>[0.6110583470536463, 0.6042969691664344, 0.586...</td>\n",
       "      <td>...</td>\n",
       "      <td>desire product software portion</td>\n",
       "      <td>risk use address show ensure possible feasible...</td>\n",
       "      <td>behave portion</td>\n",
       "      <td>address show ensure program prototype</td>\n",
       "      <td>desire behave product</td>\n",
       "      <td>risk use address program high show ensure poss...</td>\n",
       "      <td>0.514959</td>\n",
       "      <td>0.277882</td>\n",
       "      <td>0.222286</td>\n",
       "      <td>0.405072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>To simulate portions of the desired final prod...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td># ::snt To simulate portions of the desired fi...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "      <td>[0.6110583470536463, 0.6042969691664344, 0.586...</td>\n",
       "      <td>...</td>\n",
       "      <td>desire product software portion</td>\n",
       "      <td>help solve simulate possible job project progr...</td>\n",
       "      <td>behave portion</td>\n",
       "      <td>solve do program you</td>\n",
       "      <td>desire behave product</td>\n",
       "      <td>specific help do easy quick solve see desire s...</td>\n",
       "      <td>0.653427</td>\n",
       "      <td>0.346444</td>\n",
       "      <td>0.390257</td>\n",
       "      <td>0.361899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>A prototype program simulates the behaviors of...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td># ::snt A prototype program simulates the beha...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "      <td>[0.6110583470536463, 0.6042969691664344, 0.586...</td>\n",
       "      <td>...</td>\n",
       "      <td>desire product software portion</td>\n",
       "      <td>allow program portion prototype product softwa...</td>\n",
       "      <td>behave portion</td>\n",
       "      <td>behave allow program portion</td>\n",
       "      <td>desire behave product</td>\n",
       "      <td>behave allow desire check product error</td>\n",
       "      <td>0.799805</td>\n",
       "      <td>0.651449</td>\n",
       "      <td>0.670622</td>\n",
       "      <td>0.591405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>Defined in the Specification phase a prototype...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td># ::snt Defined in the Specification phase a p...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "      <td>[0.6110583470536463, 0.6042969691664344, 0.586...</td>\n",
       "      <td>...</td>\n",
       "      <td>desire product software portion</td>\n",
       "      <td>specific use define refine mean stimulate solv...</td>\n",
       "      <td>behave portion</td>\n",
       "      <td>behave stimulate solve prototype portion</td>\n",
       "      <td>desire behave product</td>\n",
       "      <td>behave use define refine stimulate solve desir...</td>\n",
       "      <td>0.490084</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>0.599031</td>\n",
       "      <td>0.349854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.1</td>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate the behaviour of portions of the d...</td>\n",
       "      <td>It is used to let the users have a first idea ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td># ::snt It is used to let the users have a fir...</td>\n",
       "      <td># ::snt To simulate the behaviour of portions ...</td>\n",
       "      <td>[0.6110583470536463, 0.6042969691664344, 0.586...</td>\n",
       "      <td>...</td>\n",
       "      <td>desire product software portion</td>\n",
       "      <td>ordinal use let estimate allow include possibl...</td>\n",
       "      <td>behave portion</td>\n",
       "      <td>have generate evaluate use let allow this it c...</td>\n",
       "      <td>desire behave product</td>\n",
       "      <td>have generate evaluate use let complete estima...</td>\n",
       "      <td>0.532519</td>\n",
       "      <td>0.307554</td>\n",
       "      <td>0.310586</td>\n",
       "      <td>0.424605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           question  \\\n",
       "0  1.1  What is the role of a prototype program in pro...   \n",
       "1  1.1  What is the role of a prototype program in pro...   \n",
       "2  1.1  What is the role of a prototype program in pro...   \n",
       "3  1.1  What is the role of a prototype program in pro...   \n",
       "4  1.1  What is the role of a prototype program in pro...   \n",
       "\n",
       "                                      desired_answer  \\\n",
       "0  To simulate the behaviour of portions of the d...   \n",
       "1  To simulate the behaviour of portions of the d...   \n",
       "2  To simulate the behaviour of portions of the d...   \n",
       "3  To simulate the behaviour of portions of the d...   \n",
       "4  To simulate the behaviour of portions of the d...   \n",
       "\n",
       "                                      student_answer  score_me  score_other  \\\n",
       "0  High risk problems are address in the prototyp...       4.0          3.0   \n",
       "1  To simulate portions of the desired final prod...       5.0          5.0   \n",
       "2  A prototype program simulates the behaviors of...       5.0          3.0   \n",
       "3  Defined in the Specification phase a prototype...       5.0          5.0   \n",
       "4  It is used to let the users have a first idea ...       3.0          3.0   \n",
       "\n",
       "   score_avg                                        student_amr  \\\n",
       "0        3.5  # ::snt High risk problems are address in the ...   \n",
       "1        5.0  # ::snt To simulate portions of the desired fi...   \n",
       "2        4.0  # ::snt A prototype program simulates the beha...   \n",
       "3        5.0  # ::snt Defined in the Specification phase a p...   \n",
       "4        3.0  # ::snt It is used to let the users have a fir...   \n",
       "\n",
       "                                           model_amr  \\\n",
       "0  # ::snt To simulate the behaviour of portions ...   \n",
       "1  # ::snt To simulate the behaviour of portions ...   \n",
       "2  # ::snt To simulate the behaviour of portions ...   \n",
       "3  # ::snt To simulate the behaviour of portions ...   \n",
       "4  # ::snt To simulate the behaviour of portions ...   \n",
       "\n",
       "                                      model_amr_ARG1  ...  \\\n",
       "0  [0.6110583470536463, 0.6042969691664344, 0.586...  ...   \n",
       "1  [0.6110583470536463, 0.6042969691664344, 0.586...  ...   \n",
       "2  [0.6110583470536463, 0.6042969691664344, 0.586...  ...   \n",
       "3  [0.6110583470536463, 0.6042969691664344, 0.586...  ...   \n",
       "4  [0.6110583470536463, 0.6042969691664344, 0.586...  ...   \n",
       "\n",
       "               model_amr_rest_text  \\\n",
       "0  desire product software portion   \n",
       "1  desire product software portion   \n",
       "2  desire product software portion   \n",
       "3  desire product software portion   \n",
       "4  desire product software portion   \n",
       "\n",
       "                               student_amr_rest_text model_amr_ARG0_text  \\\n",
       "0  risk use address show ensure possible feasible...      behave portion   \n",
       "1  help solve simulate possible job project progr...      behave portion   \n",
       "2  allow program portion prototype product softwa...      behave portion   \n",
       "3  specific use define refine mean stimulate solv...      behave portion   \n",
       "4  ordinal use let estimate allow include possibl...      behave portion   \n",
       "\n",
       "                               student_amr_ARG0_text    model_amr_ARG1_text  \\\n",
       "0              address show ensure program prototype  desire behave product   \n",
       "1                               solve do program you  desire behave product   \n",
       "2                       behave allow program portion  desire behave product   \n",
       "3           behave stimulate solve prototype portion  desire behave product   \n",
       "4  have generate evaluate use let allow this it c...  desire behave product   \n",
       "\n",
       "                               student_amr_ARG1_text similarity_default  \\\n",
       "0  risk use address program high show ensure poss...           0.514959   \n",
       "1  specific help do easy quick solve see desire s...           0.653427   \n",
       "2            behave allow desire check product error           0.799805   \n",
       "3  behave use define refine stimulate solve desir...           0.490084   \n",
       "4  have generate evaluate use let complete estima...           0.532519   \n",
       "\n",
       "   similarity_ARG1 similarity_ARG0 similarity_rest  \n",
       "0         0.277882        0.222286        0.405072  \n",
       "1         0.346444        0.390257        0.361899  \n",
       "2         0.651449        0.670622        0.591405  \n",
       "3         0.682992        0.599031        0.349854  \n",
       "4         0.307554        0.310586        0.424605  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950268a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr['similarity_default']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
